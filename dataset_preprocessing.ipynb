{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cells for Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iL10kckWVsh1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "runningFromColab = False\n",
        "if 'CGROUP_MEMORY_EVENTS' in os.environ and 'colab' in os.environ['CGROUP_MEMORY_EVENTS']:\n",
        "  runningFromColab = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTIJGz2iHMO2",
        "outputId": "326dfca4-9445-4816-911e-b765a922e293"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "if runningFromColab:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of0Zg47kK5c6",
        "outputId": "63bb8d76-b03d-4416-d307-7316ca5bb0fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/ColabProjects\n"
          ]
        }
      ],
      "source": [
        "if runningFromColab:\n",
        "  %cd /content/drive/MyDrive/ColabProjects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzrpiAjyLPrb",
        "outputId": "7ee9cf1d-4a35-4374-fbf3-d226e19ffc11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'nlp-image-captioning' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "if runningFromColab:\n",
        "  !git clone https://github.com/OdyAsh/nlp-image-captioning.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgJWSx0ENHfi",
        "outputId": "b932304f-617a-4fa5-b586-011925faa17b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/ColabProjects/nlp-image-captioning\n"
          ]
        }
      ],
      "source": [
        "if runningFromColab:\n",
        "  %cd /content/drive/MyDrive/ColabProjects/nlp-image-captioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oggUYhWqPZMX",
        "outputId": "1e4791d0-9831-47ab-a2ed-5cd18121ae74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "if runningFromColab:\n",
        "  !git pull\n",
        "  # if it doesn't say \"Already up to date.\", then you need to close this notebook file (i.e., the browser tab) and open it again for it to change "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLTA8a3-Q7ph",
        "outputId": "bef4bb2c-d71a-49bb-f060-0e352def13ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è¨ Downloading https://github.com/jaimergp/miniforge/releases/latest/download/Mambaforge-colab-Linux-x86_64.sh...\n",
            "üì¶ Installing...\n",
            "üìå Adjusting configuration...\n",
            "ü©π Patching environment...\n",
            "‚è≤ Done in 0:00:26\n",
            "üîÅ Restarting kernel...\n"
          ]
        }
      ],
      "source": [
        "if runningFromColab:\n",
        "  try:\n",
        "    import condacolab\n",
        "    condacolab.install()\n",
        "  except:\n",
        "    !pip install -q condacolab\n",
        "    import condacolab\n",
        "    condacolab.install()\n",
        "    # now restart the kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW1oaGvGMsvN",
        "outputId": "8722ca24-ba6c-42ca-8be3-d8c103cd3897"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "CondaValueError: could not parse 'name: .conda' in: environment.yml\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if runningFromColab:\n",
        "  !conda env create -f environment.yml\n",
        "  # !conda update conda -y -q\n",
        "  # !source /usr/local/etc/profile.d/conda.sh\n",
        "  # !conda init \n",
        "  # !conda install -n root _license -y -q\n",
        "  # !source activate myenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zUUVjcRLxjd"
      },
      "outputs": [],
      "source": [
        "if runningFromColab:\n",
        "  import sys\n",
        "  sys.path.insert(0, '/usr/local/bin/conda')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Courese Work 1 Requirements\n",
        "\n",
        "<img src=\"project_media/cw1_requirements.png\" width=\"500\" />"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gjH1mCsmHKZR"
      },
      "source": [
        "# Imports & Global Functions/Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "from glob import glob\n",
        "from time import time\n",
        "import os\n",
        "import pickle\n",
        "import regex as re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
        "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow.keras.layers import Add # merge.add\n",
        "from tensorflow.keras.applications import inception_v3 # inception_v3.preprocess_input\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras import preprocessing # preprocessing.image, preprocessing.sequence, preprocessing.text.Tokenizer, preprocessing.sequence.pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import Input, layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pklSave(contentToBeSaved, fullPath):\n",
        "    with open(fullPath, 'wb') as f:\n",
        "        pickle.dump(contentToBeSaved, f)\n",
        "\n",
        "def pklLoad(fullPath):\n",
        "    with open(fullPath, 'rb') as f:\n",
        "        content = pickle.load(f)\n",
        "    return content\n",
        "\n",
        "def pklForceLoad(path, dtype = 'dict'):\n",
        "    try:\n",
        "        content = pklLoad(path)\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        if dtype == 'list':\n",
        "            pklSave([], path)\n",
        "            return []\n",
        "        else:\n",
        "            pklSave({}, path)\n",
        "            return {}\n",
        "\n",
        "# more about naming standards for path components here: https://stackoverflow.com/questions/2235173/what-is-the-naming-standard-for-path-components\n",
        "def joinPaths(baseDirectory, relativePath):\n",
        "    return os.path.normpath(os.path.join(baseDirectory, relativePath))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8091"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datasetImgsBasePath = 'dataset/Flicker8k_Dataset/'\n",
        "fullImgsPath = glob(datasetImgsBasePath + '*.jpg')\n",
        "fullImgsPaths = [os.path.normpath(path) for path in fullImgsPath]\n",
        "len(fullImgsPaths)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Collection\n",
        "The dataset is obtained from [here](https://forms.illinois.edu/sec/1713398)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "first image's captions:\n",
            "['1000268201_693b08cb0e.jpg#0\\tA child in a pink dress is climbing up a set of '\n",
            " 'stairs in an entry way .',\n",
            " '1000268201_693b08cb0e.jpg#1\\tA girl going into a wooden building .',\n",
            " '1000268201_693b08cb0e.jpg#2\\tA little girl climbing into a wooden playhouse '\n",
            " '.',\n",
            " '1000268201_693b08cb0e.jpg#3\\tA little girl climbing the stairs to her '\n",
            " 'playhouse .',\n",
            " '1000268201_693b08cb0e.jpg#4\\tA little girl in a pink dress going into a '\n",
            " 'wooden cabin .']\n",
            "\n",
            "second image's captions:\n",
            "['1001773457_577c3a7d70.jpg#0\\tA black dog and a spotted dog are fighting',\n",
            " '1001773457_577c3a7d70.jpg#1\\tA black dog and a tri-colored dog playing with '\n",
            " 'each other on the road .',\n",
            " '1001773457_577c3a7d70.jpg#2\\tA black dog and a white dog with brown spots '\n",
            " 'are staring at each other in the street .',\n",
            " '1001773457_577c3a7d70.jpg#3\\tTwo dogs of different breeds looking at each '\n",
            " 'other on the road .',\n",
            " '1001773457_577c3a7d70.jpg#4\\tTwo dogs on pavement moving toward each other .']\n",
            "\n",
            "and so forth...\n"
          ]
        }
      ],
      "source": [
        "# checking the 5 captions per image\n",
        "filename = \"dataset/Flicker8k_TextFiles/Flickr8k.token.txt\"\n",
        "with open(filename, 'r') as f:\n",
        "    doc = f.read()\n",
        "lines = doc.split('\\n')\n",
        "print('first image\\'s captions:')\n",
        "pprint(lines[:5])\n",
        "print('\\nsecond image\\'s captions:')\n",
        "pprint(lines[5:10])\n",
        "print('\\nand so forth...')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The captions above are for these two images:\n",
        "\n",
        "<img src=\"project_media/1000268201_693b08cb0e.jpg\" width=\"100\" />\n",
        "\n",
        "<img src=\"project_media/1001773457_577c3a7d70.jpg\" width=\"150\" />"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['A child in a pink dress is climbing up a set of stairs in an entry way .',\n",
              " 'A girl going into a wooden building .',\n",
              " 'A little girl climbing into a wooden playhouse .',\n",
              " 'A little girl climbing the stairs to her playhouse .',\n",
              " 'A little girl in a pink dress going into a wooden cabin .']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# getting these captions in a dictionary; where the key is the image's name (without .jpg) and the value is a list of 5 captions\n",
        "imgToCaptions = dict()\n",
        "for line in lines:\n",
        "    idAndCaption = re.split(\"\\..+\\t\", line)\n",
        "    if len(idAndCaption) < 2:\n",
        "        continue\n",
        "    imgId, caption = idAndCaption\n",
        "    if imgId not in imgToCaptions:\n",
        "        imgToCaptions[imgId] = list()\n",
        "    imgToCaptions[imgId].append(caption)\n",
        "    \n",
        "imgToCaptions['1000268201_693b08cb0e']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['child in pink dress is climbing up set of stairs in entry way',\n",
              " 'girl going into wooden building',\n",
              " 'little girl climbing into wooden playhouse',\n",
              " 'little girl climbing stairs to her playhouse',\n",
              " 'little girl in pink dress going into wooden cabin']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# removing punctuation using maketrans (i.e., translation table)\n",
        "# more about maketrans method: https://www.w3schools.com/python/ref_string_maketrans.asp#:~:text=The%20third%20parameter%20in%20the%20mapping%20table%20describes%20characters%20that%20you%20want%20to%20remove%20from%20the%20string%3A\n",
        "\n",
        "# to do: ASK Dr: should I remove numbers/stopwords for image captioning task?\n",
        "\n",
        "def cleanCaptions(imgToCaptions, levelOfStopwordsPresence=1):\n",
        "    table = str.maketrans('', '', string.punctuation) # third argument: removes any character in this list: '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "    for key, descList in imgToCaptions.items():\n",
        "        # when this for loop is done, all 5 captions of an image will be cleaned\n",
        "        for i in range(len(descList)):\n",
        "            desc = descList[i]\n",
        "            desc = desc.split(' ')\n",
        "            desc = [word.lower() for word in desc]\n",
        "            desc = [word.translate(table) for word in desc] # remove punctuation from each token\n",
        "            stopswordsToRemove = []\n",
        "            if levelOfStopwordsPresence == 1:\n",
        "                stopswordsToRemove = ['a', 'an', 'the']\n",
        "            elif levelOfStopwordsPresence >= 2:\n",
        "                stopswordsToRemove = set(stopwords.words('english'))\n",
        "            desc = [word for word in desc if word not in stopswordsToRemove]\n",
        "            desc = [word for word in desc if word.isalpha()] # remove tokens with numbers in them\n",
        "            descList[i] =  ' '.join(desc) # store as string\n",
        "\n",
        "# cleanCaptions(imgToCaptions, levelOfStopwordsPresence=1)\n",
        "# pklSave(imgToCaptions, 'dataset/pickles/imgToCaptionsSWKept.pickle')\n",
        "imgToCaptions = pklLoad('dataset/pickles/imgToCaptionsSWKept.pickle')\n",
        "imgToCaptions['1000268201_693b08cb0e']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "example with stopwords removed:\n",
        "<br><br>\n",
        "'little girl climbing stairs playhouse',\n",
        "\n",
        "<br>\n",
        "example with only ['a', 'an', 'the'] removed:\n",
        "<br><br>\n",
        "'little girl climbing stairs to her playhouse',\n",
        "<br><br>\n",
        "from the lack of context seen above, we've decided to keep the rest of the stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original vocabulary (i.e., unique words) size: 8366\n",
            "Vocabulary size after removing less frequent words (< 5 words): 2945\n"
          ]
        }
      ],
      "source": [
        "# creating vocab of unique words (where each word occured at least freqThreshold number of times)\n",
        "def createVocab(freqThreshold = 10):\n",
        "    vocab = set()\n",
        "    for key in imgToCaptions.keys():\n",
        "        [vocab.update(desc.split()) for desc in imgToCaptions[key]]\n",
        "    print(f'Original vocabulary (i.e., unique words) size: {len(vocab)}')\n",
        "\n",
        "    # keeping words that appear at least freqThrehold number of times\n",
        "    # ASK DR: should i do that? or retain all words? should this be considered a hyperparameter?\n",
        "    vocabWordFreq = {key: 0 for key in vocab}\n",
        "    for key, descs in imgToCaptions.items():\n",
        "        for desc in descs:\n",
        "            descList = desc.split(' ')\n",
        "            for word in descList:\n",
        "                if word != '':\n",
        "                    vocabWordFreq[word] += 1\n",
        "    \n",
        "    vocab = set()\n",
        "    vocabWordFreqFinal = dict()\n",
        "    i = 0\n",
        "    for word, freq in vocabWordFreq.items():\n",
        "        if freq >= freqThreshold:\n",
        "            i += 1\n",
        "            vocab.add(word)\n",
        "            vocabWordFreqFinal[word] = freq\n",
        "    print(f'Vocabulary size after removing less frequent words (< {freqThreshold} words): {len(vocab)}')\n",
        "\n",
        "    vocabWordFreqRemoved = {word: freq for word, freq in vocabWordFreq.items() if word not in vocabWordFreqFinal}\n",
        "\n",
        "    return vocab, vocabWordFreqFinal, vocabWordFreqRemoved\n",
        "\n",
        "def createVocabTxtFiles(vocabWordFreqFinal, vocabWordFreqRemoved, filePrefix=\"vocabFreqThreshold\"):\n",
        "    with open(f'dataset/{filePrefix}Final.txt', 'w') as f:\n",
        "        f.write(str(dict(sorted(vocabWordFreqFinal.items(), key=lambda x: x[1], reverse=True))))\n",
        "    with open(f'dataset/{filePrefix}Removed.txt', 'w') as f:\n",
        "        f.write(str(dict(sorted(vocabWordFreqRemoved.items(), key=lambda x: x[1], reverse=True))))\n",
        "\n",
        "vocab, vocabWordFreqFinal, vocabWordFreqRemoved = createVocab(freqThreshold=5)\n",
        "createVocabTxtFiles(vocabWordFreqFinal, vocabWordFreqRemoved, filePrefix=f\"vocabFreqThreshold5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Dataset: 6000\n",
            "Validation Dataset: 1000\n",
            "Test Dataset: 1000\n"
          ]
        }
      ],
      "source": [
        "#  function to get filenames of images from a text file (without the extension)\n",
        "def getImgsIdsList(txtPath):\n",
        "    with open(txtPath, 'r') as f:\n",
        "        doc = f.read()\n",
        "    ImgsIds = []\n",
        "    for line in doc.split('\\n'):\n",
        "        imgId = line.split('.')[0]\n",
        "        ImgsIds.append(imgId)\n",
        "    ImgsIds = [id for id in ImgsIds if id != '']\n",
        "    return ImgsIds\n",
        "\n",
        "trainImgsIds = getImgsIdsList('dataset/Flicker8k_TextFiles/Flickr_8k.trainImages.txt')\n",
        "valImgsIds = getImgsIdsList('dataset/Flicker8k_TextFiles/Flickr_8k.devImages.txt')\n",
        "testImgsIds = getImgsIdsList('dataset/Flicker8k_TextFiles/Flickr_8k.testImages.txt')\n",
        "print(f'Train Dataset: {len(trainImgsIds)}')\n",
        "print(f'Validation Dataset: {len(valImgsIds)}')\n",
        "print(f'Test Dataset: {len(testImgsIds)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "images in training set: 6000\n",
            "\n",
            "images in validation set: 1000\n",
            "\n",
            "images in testing set: 1000\n",
            "\n",
            "example from training set:\n",
            "['startseq black dog is running after white dog in snow endseq',\n",
            " 'startseq black dog chasing brown dog through snow endseq',\n",
            " 'startseq two dogs chase each other across snowy ground endseq',\n",
            " 'startseq two dogs play together in snow endseq',\n",
            " 'startseq two dogs running through low lying body of water endseq']\n"
          ]
        }
      ],
      "source": [
        "trainImgToCaptions = dict()\n",
        "valImgToCaptions = dict()\n",
        "testImgToCaptions = dict()\n",
        "for imgId in trainImgsIds:\n",
        "    trainImgToCaptions[imgId] = ['startseq ' + desc + ' endseq' for desc in imgToCaptions[imgId]]\n",
        "for imgId in valImgsIds:\n",
        "    valImgToCaptions[imgId] = ['startseq ' + desc + ' endseq' for desc in imgToCaptions[imgId]]\n",
        "for imgId in testImgsIds:\n",
        "    testImgToCaptions[imgId] = ['startseq ' + desc + ' endseq' for desc in imgToCaptions[imgId]]\n",
        "print(f'images in training set: {len(trainImgToCaptions)}\\n')\n",
        "print(f'images in validation set: {len(valImgToCaptions)}\\n')\n",
        "print(f'images in testing set: {len(testImgToCaptions)}\\n')\n",
        "print('example from training set:')\n",
        "pprint(trainImgToCaptions['2513260012_03d33305cf'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preparing File Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6000, 1000, 1000)"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainImgsPaths = [joinPaths(datasetImgsBasePath, imgId)+'.jpg' for imgId in trainImgToCaptions.keys()]\n",
        "valImgsPaths = [joinPaths(datasetImgsBasePath, imgId)+'.jpg' for imgId in valImgToCaptions.keys()]\n",
        "testImgsPaths = [joinPaths(datasetImgsBasePath, imgId)+'.jpg' for imgId in testImgToCaptions.keys()]\n",
        "len(trainImgsPaths), len(valImgsPaths), len(testImgsPaths)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Pre-Processing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre-Processing Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 1000) dtype=float32 (created by layer 'predictions')>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# getting the feature vector of each image using the InceptionV3 CNN model created by Google Research\n",
        "model = InceptionV3(weights='imagenet') # getting the InceptionV3 model trained on imagenet data\n",
        "model.layers[-1].output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 2048) dtype=float32 (created by layer 'avg_pool')>"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "modelForFeatureExtraction = Model(model.input, model.layers[-2].output) # removing the last layer (output softmax layer)\n",
        "modelForFeatureExtraction.layers[-1].output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to preprocess the input image\n",
        "def preprocess(imgPath):\n",
        "    pilImg = preprocessing.image.load_img(imgPath, target_size=(299, 299)) # Convert all the images to size 299x299 as expected by the inception v3 model\n",
        "    x = preprocessing.image.img_to_array(pilImg) # Convert PIL image to numpy array of 3-dimensions\n",
        "    x = np.expand_dims(x, axis=0) # Add one more dimension; from (299, 299, 3) to (1, 299, 299, 3)\n",
        "    x = inception_v3.preprocess_input(x) # takes in (batch_size, height, width, channels), returns same dimensions, but does some preprocessing operations, like scaling values to be from -1 to 1\n",
        "    return x\n",
        "\n",
        "# function to encode a given image (from its path) into a vector of size (2048, )\n",
        "def encode(imgPath):\n",
        "    imgPath = preprocess(imgPath) # preprocess the image\n",
        "    featureVec = modelForFeatureExtraction.predict(imgPath) # Get the encoding vector for the image\n",
        "    featureVec = np.reshape(featureVec, featureVec.shape[1]) # reshape from (1, 2048) to (2048, )\n",
        "    return featureVec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6000, 1000, 1000, (2048,))"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Call the funtion to encode all the train images (dictionary where an image id --> feature vector of length 2048)\n",
        "# This will take a while on CPU - Execute this only once (took around 13 minutes on my high-end laptop)\n",
        "def encodeImgToFeatures(imgsPaths):\n",
        "    imgToFeatures = dict()\n",
        "    for imgPath in imgsPaths:\n",
        "        imgToFeatures[imgPath[len(datasetImgsBasePath):]] = encode(imgPath)\n",
        "    return imgToFeatures\n",
        "\n",
        "# trainImgToFeatures = encodeImgToFeatures(trainImgsPaths)\n",
        "# valImgToFeatures = encodeImgToFeatures(valImgsPaths)\n",
        "# testImgToFeatures = encodeImgToFeatures(testImgsPaths)\n",
        "# pklSave(trainImgToFeatures, 'dataset/pickles/trainImgToFeatures.pickle')\n",
        "# pklSave(valImgToFeatures, 'dataset/pickles/valImgToFeatures.pickle')\n",
        "# pklSave(testImgToFeatures, 'dataset/pickles/testImgToFeatures.pickle')\n",
        "trainImgToFeatures = pklLoad('dataset/pickles/trainImgToFeatures.pickle')\n",
        "valImgToFeatures = pklLoad('dataset/pickles/valImgToFeatures.pickle')\n",
        "testImgToFeatures = pklLoad('dataset/pickles/testImgToFeatures.pickle')\n",
        "len(trainImgToFeatures), len(valImgToFeatures), len(testImgToFeatures), trainImgToFeatures['2513260012_03d33305cf.jpg'].shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre-Processing Captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original vocabulary (i.e., unique words) size: 8366\n",
            "Vocabulary size after removing less frequent words (< 5 words): 2945\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2946"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# creating two dictionaries: word to index, and index to word\n",
        "\n",
        "def mapIdxAndWord(vocab):\n",
        "    idxToWord = {}\n",
        "    wordToIdx = {}\n",
        "    idx = 1\n",
        "    for word in vocab:\n",
        "        wordToIdx[word] = idx\n",
        "        idxToWord[idx] = word\n",
        "        idx += 1\n",
        "    return idxToWord, wordToIdx\n",
        "\n",
        "vocab, _, _ = createVocab(freqThreshold=5)\n",
        "idxToWord, wordToIdx = mapIdxAndWord(vocab)\n",
        "vocabSize = len(idxToWord) + 1 # one for appended 0's; explained later\n",
        "vocabSize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Description Length: 32\n"
          ]
        }
      ],
      "source": [
        "# getting the length of the longest caption; as we will later need to encode each word into a fixed sized vector\n",
        "\n",
        "# convert a dictionary of clean captions to a list of captions\n",
        "def toCaptionsList(ImgToCaptions):\n",
        "\tcaptionsList = list()\n",
        "\tfor imgId in ImgToCaptions.keys():\n",
        "\t\t[captionsList.append(caption) for caption in ImgToCaptions[imgId]]\n",
        "\treturn captionsList\n",
        "\n",
        "# calculate the length of the description with the most words\n",
        "def maxCaptionLength(ImgToCaptions):\n",
        "    captions = toCaptionsList(ImgToCaptions)\n",
        "    captionsLengths = [len(caption.split()) for caption in captions]\n",
        "    return max(captionsLengths)\n",
        "\n",
        "# determine the maximum sequence length\n",
        "maxCapLen = maxCaptionLength(trainImgToCaptions)\n",
        "print(f'Description Length: {maxCapLen}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c7cdde67bd1df58aee8e05336b4c9e41dfb37650659bf34d4c10242d0898ac5a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
