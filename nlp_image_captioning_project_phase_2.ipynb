{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Side note: all the cells until the `Pipelining Preprocessing Steps` are similar to the cells in `...phase_1.ipynb` file, as we've worked on preprocessing steps to be used in phase 2 when writing the code for phase 1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cells for Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "iL10kckWVsh1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "runningFromColab = False\n",
        "if 'CGROUP_MEMORY_EVENTS' in os.environ and 'colab' in os.environ['CGROUP_MEMORY_EVENTS']:\n",
        "  runningFromColab = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTIJGz2iHMO2",
        "outputId": "326dfca4-9445-4816-911e-b765a922e293"
      },
      "outputs": [],
      "source": [
        "if runningFromColab:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of0Zg47kK5c6",
        "outputId": "63bb8d76-b03d-4416-d307-7316ca5bb0fc"
      },
      "outputs": [],
      "source": [
        "if runningFromColab:\n",
        "  %cd /content/drive/MyDrive/ColabProjects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzrpiAjyLPrb",
        "outputId": "7ee9cf1d-4a35-4374-fbf3-d226e19ffc11"
      },
      "outputs": [],
      "source": [
        "if runningFromColab:\n",
        "  !git clone https://github.com/OdyAsh/nlp-image-captioning.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgJWSx0ENHfi",
        "outputId": "b932304f-617a-4fa5-b586-011925faa17b"
      },
      "outputs": [],
      "source": [
        "if runningFromColab:\n",
        "  %cd /content/drive/MyDrive/ColabProjects/nlp-image-captioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oggUYhWqPZMX",
        "outputId": "1e4791d0-9831-47ab-a2ed-5cd18121ae74"
      },
      "outputs": [],
      "source": [
        "if runningFromColab:\n",
        "  !git pull\n",
        "  # if it DOES NOT say \"Already up to date.\", then you need to close this notebook file (i.e., the browser tab) and open it again for it to change "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLTA8a3-Q7ph",
        "outputId": "bef4bb2c-d71a-49bb-f060-0e352def13ab"
      },
      "outputs": [],
      "source": [
        "# if runningFromColab:\n",
        "#   try:\n",
        "#     import condacolab\n",
        "#     condacolab.install()\n",
        "#   except:\n",
        "#     !pip install -q condacolab\n",
        "#     import condacolab\n",
        "#     condacolab.install()\n",
        "#     # now restart the kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW1oaGvGMsvN",
        "outputId": "8722ca24-ba6c-42ca-8be3-d8c103cd3897"
      },
      "outputs": [],
      "source": [
        "# if runningFromColab:\n",
        "#   !conda env create -f environment.yml\n",
        "#   # !conda update conda -y -q\n",
        "#   # !source /usr/local/etc/profile.d/conda.sh\n",
        "#   # !conda init \n",
        "#   # !conda install -n root _license -y -q\n",
        "#   # !source activate myenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "7zUUVjcRLxjd"
      },
      "outputs": [],
      "source": [
        "# if runningFromColab:\n",
        "#   import sys\n",
        "#   sys.path.insert(0, '/usr/local/bin/conda')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gjH1mCsmHKZR"
      },
      "source": [
        "# Imports & Global Functions/Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "from glob import glob\n",
        "from time import time\n",
        "import os\n",
        "import pickle\n",
        "import regex as re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
        "#                          Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
        "# from tensorflow.keras.layers import Bidirectional\n",
        "# from tensorflow.keras.layers import Add # merge.add\n",
        "from tensorflow.keras.applications import inception_v3 # inception_v3.preprocess_input\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras import preprocessing # preprocessing.image, preprocessing.sequence, preprocessing.text.Tokenizer, preprocessing.sequence.pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import Input, layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ashra\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pklSave(contentToBeSaved, fullPath):\n",
        "    with open(fullPath, 'wb') as f:\n",
        "        pickle.dump(contentToBeSaved, f)\n",
        "\n",
        "def pklLoad(fullPath):\n",
        "    with open(fullPath, 'rb') as f:\n",
        "        content = pickle.load(f)\n",
        "    return content\n",
        "\n",
        "def pklForceLoad(path, dtype = 'dict'):\n",
        "    try:\n",
        "        content = pklLoad(path)\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        if dtype == 'list':\n",
        "            pklSave([], path)\n",
        "            return []\n",
        "        else:\n",
        "            pklSave({}, path)\n",
        "            return {}\n",
        "\n",
        "# more about naming standards for path components here: https://stackoverflow.com/questions/2235173/what-is-the-naming-standard-for-path-components\n",
        "def joinPaths(baseDirectory, relativePath):\n",
        "    return os.path.normpath(os.path.join(baseDirectory, relativePath))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8091"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datasetImgsBasePath = 'dataset/Flicker8k_Dataset/'\n",
        "fullImgsPath = glob(datasetImgsBasePath + '*.jpg')\n",
        "fullImgsPaths = [os.path.normpath(path) for path in fullImgsPath]\n",
        "len(fullImgsPaths)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Collection\n",
        "The dataset is obtained from [here](https://forms.illinois.edu/sec/1713398)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "first image's captions:\n",
            "['1000268201_693b08cb0e.jpg#0\\tA child in a pink dress is climbing up a set of '\n",
            " 'stairs in an entry way .',\n",
            " '1000268201_693b08cb0e.jpg#1\\tA girl going into a wooden building .',\n",
            " '1000268201_693b08cb0e.jpg#2\\tA little girl climbing into a wooden playhouse '\n",
            " '.',\n",
            " '1000268201_693b08cb0e.jpg#3\\tA little girl climbing the stairs to her '\n",
            " 'playhouse .',\n",
            " '1000268201_693b08cb0e.jpg#4\\tA little girl in a pink dress going into a '\n",
            " 'wooden cabin .']\n",
            "\n",
            "second image's captions:\n",
            "['1001773457_577c3a7d70.jpg#0\\tA black dog and a spotted dog are fighting',\n",
            " '1001773457_577c3a7d70.jpg#1\\tA black dog and a tri-colored dog playing with '\n",
            " 'each other on the road .',\n",
            " '1001773457_577c3a7d70.jpg#2\\tA black dog and a white dog with brown spots '\n",
            " 'are staring at each other in the street .',\n",
            " '1001773457_577c3a7d70.jpg#3\\tTwo dogs of different breeds looking at each '\n",
            " 'other on the road .',\n",
            " '1001773457_577c3a7d70.jpg#4\\tTwo dogs on pavement moving toward each other .']\n",
            "\n",
            "and so forth...\n"
          ]
        }
      ],
      "source": [
        "# checking the 5 captions per image\n",
        "filename = \"dataset/Flicker8k_TextFiles/Flickr8k.token.txt\"\n",
        "with open(filename, 'r') as f:\n",
        "    doc = f.read()\n",
        "lines = doc.split('\\n')\n",
        "print('first image\\'s captions:')\n",
        "pprint(lines[:5])\n",
        "print('\\nsecond image\\'s captions:')\n",
        "pprint(lines[5:10])\n",
        "print('\\nand so forth...')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The captions above are for these two images:\n",
        "\n",
        "<img src=\"project_media/1000268201_693b08cb0e.jpg\" width=\"100\" />\n",
        "\n",
        "<img src=\"project_media/1001773457_577c3a7d70.jpg\" width=\"150\" />"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Cleaning\n",
        "Includes:\n",
        "* `imgToCaptions` dictionary\n",
        "* `cleanCaptions()` to remove stopwords/punctuations\n",
        "* `createVocab()` to limit vocab size based on word frequency\n",
        "* `train`, `val`, and `test` `ImgToCaptions` which prepends `startseq` and appends `endseq`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['A child in a pink dress is climbing up a set of stairs in an entry way .',\n",
              " 'A girl going into a wooden building .',\n",
              " 'A little girl climbing into a wooden playhouse .',\n",
              " 'A little girl climbing the stairs to her playhouse .',\n",
              " 'A little girl in a pink dress going into a wooden cabin .']"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# getting these captions in a dictionary; \n",
        "# where the key is the image's name (without .jpg) and the value is a list of 5 captions\n",
        "\n",
        "imgToCaptions = dict()\n",
        "for line in lines:\n",
        "    idAndCaption = re.split(\"\\..+\\t\", line)\n",
        "    if len(idAndCaption) < 2:\n",
        "        continue\n",
        "    imgId, caption = idAndCaption\n",
        "    if imgId not in imgToCaptions:\n",
        "        imgToCaptions[imgId] = list()\n",
        "    imgToCaptions[imgId].append(caption)\n",
        "    \n",
        "imgToCaptions['1000268201_693b08cb0e']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['child in pink dress is climbing up set of stairs in entry way',\n",
              " 'girl going into wooden building',\n",
              " 'little girl climbing into wooden playhouse',\n",
              " 'little girl climbing stairs to her playhouse',\n",
              " 'little girl in pink dress going into wooden cabin']"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# removing punctuation using maketrans (i.e., translation table)\n",
        "# more about maketrans method: https://www.w3schools.com/python/ref_string_maketrans.asp#:~:text=The%20third%20parameter%20in%20the%20mapping%20table%20describes%20characters%20that%20you%20want%20to%20remove%20from%20the%20string%3A\n",
        "\n",
        "# to do: ASK Dr: We've removed few stopwords in order for generated caption to make sense, is this logical?\n",
        "#                We've also removed numbers\n",
        "\n",
        "def cleanCaptions(imgToCaptions, levelOfStopwordsPresence=1):\n",
        "    table = str.maketrans('', '', string.punctuation) # third argument: removes any character in this list: '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "    for key, descList in imgToCaptions.items():\n",
        "        # when this for loop is done, all 5 captions of an image will be cleaned\n",
        "        for i in range(len(descList)):\n",
        "            desc = descList[i]\n",
        "            desc = desc.split(' ')\n",
        "            desc = [word.lower() for word in desc]\n",
        "            desc = [word.translate(table) for word in desc] # remove punctuation from each token\n",
        "            stopswordsToRemove = []\n",
        "            if levelOfStopwordsPresence == 1:\n",
        "                stopswordsToRemove = ['a', 'an', 'the']\n",
        "            elif levelOfStopwordsPresence >= 2:\n",
        "                stopswordsToRemove = set(stopwords.words('english'))\n",
        "            desc = [word for word in desc if word not in stopswordsToRemove]\n",
        "            desc = [word for word in desc if word.isalpha()] # remove tokens with numbers in them\n",
        "            descList[i] =  ' '.join(desc) # store as string\n",
        "\n",
        "# cleanCaptions(imgToCaptions, levelOfStopwordsPresence=1)\n",
        "# pklSave(imgToCaptions, 'dataset/pickles/imgToCaptionsSWKept.pickle')\n",
        "imgToCaptions = pklLoad('dataset/pickles/imgToCaptionsSWKept.pickle')\n",
        "imgToCaptions['1000268201_693b08cb0e']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "example with stopwords removed:\n",
        "<br><br>\n",
        "'little girl climbing stairs playhouse',\n",
        "\n",
        "<br>\n",
        "example with only ['a', 'an', 'the'] removed:\n",
        "<br><br>\n",
        "'little girl climbing stairs to her playhouse',\n",
        "<br><br>\n",
        "from the lack of context seen above, we've decided to keep the rest of the stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original vocabulary (i.e., unique words) size: 8366\n",
            "Vocabulary size after removing less frequent words (< 5 words): 2945\n"
          ]
        }
      ],
      "source": [
        "# creating vocab of unique words (where each word occured at least freqThreshold number of times)\n",
        "def createVocab(imgToCaptions, freqThreshold = 10):\n",
        "    vocab = set()\n",
        "    for key in imgToCaptions.keys():\n",
        "        [vocab.update(desc.split()) for desc in imgToCaptions[key]]\n",
        "    print(f'Original vocabulary (i.e., unique words) size: {len(vocab)}')\n",
        "\n",
        "    # keeping words that appear at least freqThrehold number of times\n",
        "    vocabWordFreq = {key: 0 for key in vocab}\n",
        "    for key, descs in imgToCaptions.items():\n",
        "        for desc in descs:\n",
        "            descList = desc.split(' ')\n",
        "            for word in descList:\n",
        "                if word != '':\n",
        "                    vocabWordFreq[word] += 1\n",
        "    \n",
        "    vocab = set()\n",
        "    i = 0\n",
        "    for word, freq in vocabWordFreq.items():\n",
        "        if freq >= freqThreshold:\n",
        "            i += 1\n",
        "            vocab.add(word)\n",
        "            vocabWordFreq[word] = freq\n",
        "    print(f'Vocabulary size after removing less frequent words (< {freqThreshold} words): {len(vocab)}')\n",
        "\n",
        "    vocabWordFreqRemoved = {word: freq for word, freq in vocabWordFreq.items() if word not in vocabWordFreq}\n",
        "\n",
        "    return vocab, vocabWordFreq, vocabWordFreqRemoved\n",
        "\n",
        "def createVocabTxtFiles(vocabWordFreq, vocabWordFreqRemoved, filePrefix=\"vocabFreqThreshold\"):\n",
        "    with open(f'dataset/{filePrefix}.txt', 'w') as f:\n",
        "        f.write(str(dict(sorted(vocabWordFreq.items(), key=lambda x: x[1], reverse=True))))\n",
        "    with open(f'dataset/{filePrefix}Removed.txt', 'w') as f:\n",
        "        f.write(str(dict(sorted(vocabWordFreqRemoved.items(), key=lambda x: x[1], reverse=True))))\n",
        "\n",
        "freqThreshold = 5\n",
        "vocab, vocabWordFreq, vocabWordFreqRemoved = createVocab(imgToCaptions, freqThreshold)\n",
        "createVocabTxtFiles(vocabWordFreq, vocabWordFreqRemoved, filePrefix=f\"vocabFreqThreshold{freqThreshold}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Dataset: 6000\n",
            "Validation Dataset: 1000\n",
            "Test Dataset: 1000\n"
          ]
        }
      ],
      "source": [
        "# function to get filenames of images from a text file (without the extension)\n",
        "def getImgsIdsList(txtPath):\n",
        "    with open(txtPath, 'r') as f:\n",
        "        doc = f.read()\n",
        "    ImgsIds = []\n",
        "    for line in doc.split('\\n'):\n",
        "        imgId = line.split('.')[0]\n",
        "        ImgsIds.append(imgId)\n",
        "    ImgsIds = [id for id in ImgsIds if id != '']\n",
        "    return ImgsIds\n",
        "\n",
        "trainImgsIds = getImgsIdsList('dataset/Flicker8k_TextFiles/Flickr_8k.trainImages.txt')\n",
        "valImgsIds = getImgsIdsList('dataset/Flicker8k_TextFiles/Flickr_8k.devImages.txt')\n",
        "testImgsIds = getImgsIdsList('dataset/Flicker8k_TextFiles/Flickr_8k.testImages.txt')\n",
        "print(f'Train Dataset: {len(trainImgsIds)}')\n",
        "print(f'Validation Dataset: {len(valImgsIds)}')\n",
        "print(f'Test Dataset: {len(testImgsIds)}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the code below, we use `startseq` and `endseq` for the following reasons:\n",
        "* startseq : Will indicate the start of the caption generation process\n",
        "* endseq : to stop predicting words as soon as it appears"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "images in training set: 6000\n",
            "\n",
            "images in validation set: 1000\n",
            "\n",
            "images in testing set: 1000\n",
            "\n",
            "example from training set:\n",
            "['startseq black dog is running after white dog in snow endseq',\n",
            " 'startseq black dog chasing brown dog through snow endseq',\n",
            " 'startseq two dogs chase each other across snowy ground endseq',\n",
            " 'startseq two dogs play together in snow endseq',\n",
            " 'startseq two dogs running through low lying body of water endseq']\n"
          ]
        }
      ],
      "source": [
        "trainImgToCaptions = dict()\n",
        "valImgToCaptions = dict()\n",
        "testImgToCaptions = dict()\n",
        "for imgId in trainImgsIds:\n",
        "    trainImgToCaptions[imgId] = ['startseq ' + desc + ' endseq' for desc in imgToCaptions[imgId]]\n",
        "for imgId in valImgsIds:\n",
        "    valImgToCaptions[imgId] = ['startseq ' + desc + ' endseq' for desc in imgToCaptions[imgId]]\n",
        "for imgId in testImgsIds:\n",
        "    testImgToCaptions[imgId] = ['startseq ' + desc + ' endseq' for desc in imgToCaptions[imgId]]\n",
        "print(f'images in training set: {len(trainImgToCaptions)}\\n')\n",
        "print(f'images in validation set: {len(valImgToCaptions)}\\n')\n",
        "print(f'images in testing set: {len(testImgToCaptions)}\\n')\n",
        "print('example from training set:')\n",
        "pprint(trainImgToCaptions['2513260012_03d33305cf'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preparing File Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6000, 1000, 1000)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainImgsPaths = [joinPaths(datasetImgsBasePath, imgId)+'.jpg' for imgId in trainImgToCaptions.keys()]\n",
        "valImgsPaths = [joinPaths(datasetImgsBasePath, imgId)+'.jpg' for imgId in valImgToCaptions.keys()]\n",
        "testImgsPaths = [joinPaths(datasetImgsBasePath, imgId)+'.jpg' for imgId in testImgToCaptions.keys()]\n",
        "len(trainImgsPaths), len(valImgsPaths), len(testImgsPaths)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Pre-Processing\n",
        "Includes:\n",
        "* Pre-Processing Images\n",
        "* Pre-Processing Captions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre-Processing Images\n",
        "Includes:\n",
        "* Loading Google's `InceptionV3` model\n",
        "* Preprocessing the image\n",
        "* Encoding the image by inputting it to `InceptionV3` to get a `2048` feature vector of the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 1000) dtype=float32 (created by layer 'predictions')>"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# getting the feature vector of each image using the InceptionV3 CNN model created by Google Research\n",
        "model = InceptionV3(weights='imagenet') # getting the InceptionV3 model trained on imagenet data\n",
        "model.layers[-1].output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 2048) dtype=float32 (created by layer 'avg_pool')>"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "modelForFeatureExtraction = Model(model.input, model.layers[-2].output) # removing the last layer (output softmax layer)\n",
        "modelForFeatureExtraction.layers[-1].output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to preprocess the input image\n",
        "def preprocess(imgPath):\n",
        "    pilImg = preprocessing.image.load_img(imgPath, target_size=(299, 299)) # Convert all the images to size 299x299 as expected by the inception v3 model\n",
        "    x = preprocessing.image.img_to_array(pilImg) # Convert PIL image to numpy array of 3-dimensions\n",
        "    x = np.expand_dims(x, axis=0) # Add one more dimension; from (299, 299, 3) to (1, 299, 299, 3)\n",
        "    x = inception_v3.preprocess_input(x) # takes in (batch_size, height, width, channels), returns same dimensions, but does some preprocessing operations, like scaling values to be from -1 to 1\n",
        "    return x\n",
        "\n",
        "# function to encode a given image (from its path) into a vector of size (2048, )\n",
        "def encode(imgPath, modelForFeatureExtraction):\n",
        "    imgPath = preprocess(imgPath) # preprocess the image\n",
        "    featureVec = modelForFeatureExtraction.predict(imgPath) # Get the encoding vector for the image\n",
        "    featureVec = np.reshape(featureVec, featureVec.shape[1]) # reshape from (1, 2048) to (2048, )\n",
        "    return featureVec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6000, 1000, 1000, (2048,))"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Call the funtion to encode all the train images (dictionary where an image id --> feature vector of length 2048)\n",
        "# This will take a while on CPU - Execute this only once (took around 13 minutes on my high-end laptop)\n",
        "def encodeImgToFeatures(imgsPaths, modelForFeatureExtraction):\n",
        "    imgToFeatures = dict()\n",
        "    for imgPath in imgsPaths:\n",
        "        imgToFeatures[imgPath[len(datasetImgsBasePath):]] = encode(imgPath, modelForFeatureExtraction)\n",
        "    return imgToFeatures\n",
        "\n",
        "# trainImgToFeatures = encodeImgToFeatures(trainImgsPaths, modelForFeatureExtraction)\n",
        "# valImgToFeatures = encodeImgToFeatures(valImgsPaths, modelForFeatureExtraction)\n",
        "# testImgToFeatures = encodeImgToFeatures(testImgsPaths, modelForFeatureExtraction)\n",
        "# pklSave(trainImgToFeatures, 'dataset/pickles/trainImgToFeatures.pickle')\n",
        "# pklSave(valImgToFeatures, 'dataset/pickles/valImgToFeatures.pickle')\n",
        "# pklSave(testImgToFeatures, 'dataset/pickles/testImgToFeatures.pickle')\n",
        "trainImgToFeatures = pklLoad('dataset/pickles/trainImgToFeatures.pickle')\n",
        "valImgToFeatures = pklLoad('dataset/pickles/valImgToFeatures.pickle')\n",
        "testImgToFeatures = pklLoad('dataset/pickles/testImgToFeatures.pickle')\n",
        "len(trainImgToFeatures), len(valImgToFeatures), len(testImgToFeatures), trainImgToFeatures['2513260012_03d33305cf.jpg'].shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre-Processing Captions\n",
        "Includes:\n",
        "* `mapIdxAndWord()` to map indices to words and vice versa, where the words are obtained from `createVocab()`\n",
        "* `maxCaptionLength()` to get the caption with the most amount of words, to be used later to pad input sequences <br> (explained in `Preparing Model Generator` section)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original vocabulary (i.e., unique words) size: 8366\n",
            "Vocabulary size after removing less frequent words (< 5 words): 2945\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2946"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# creating two dictionaries: word to index, and index to word\n",
        "\n",
        "def mapIdxAndWord(vocab):\n",
        "    idxToWord = {}\n",
        "    wordToIdx = {}\n",
        "    idx = 1\n",
        "    for word in vocab:\n",
        "        wordToIdx[word] = idx\n",
        "        idxToWord[idx] = word\n",
        "        idx += 1\n",
        "    return idxToWord, wordToIdx\n",
        "\n",
        "vocab, _, _ = createVocab(imgToCaptions, freqThreshold=5)\n",
        "idxToWord, wordToIdx = mapIdxAndWord(vocab)\n",
        "vocabSize = len(idxToWord) + 1 # one for appended 0's; represents \"startseq\" (explained in \"Preparing Model Generator\" section)\n",
        "vocabSize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Description Length: 32\n"
          ]
        }
      ],
      "source": [
        "# getting the length of the longest caption; as we will later need to encode each word into a fixed sized vector\n",
        "\n",
        "# convert a dictionary of clean captions to a list of captions\n",
        "def toCaptionsList(ImgToCaptions):\n",
        "\tcaptionsList = list()\n",
        "\tfor imgId in ImgToCaptions.keys():\n",
        "\t\t[captionsList.append(caption) for caption in ImgToCaptions[imgId]]\n",
        "\treturn captionsList\n",
        "\n",
        "# calculate the length of the description with the most words\n",
        "def maxCaptionLength(ImgToCaptions):\n",
        "    captions = toCaptionsList(ImgToCaptions)\n",
        "    captionsLengths = [len(caption.split()) for caption in captions]\n",
        "    return max(captionsLengths)\n",
        "\n",
        "# determine the maximum sequence length\n",
        "maxCapLen = maxCaptionLength(trainImgToCaptions)\n",
        "print(f'Description Length: {maxCapLen}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "maxCaptionLength(testImgToCaptions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Phase 2 to do list:\n",
        "1. create function that pipelines all above pre-processing steps with parameters that allow for hyperparameter tuning\n",
        "2. use Glove embedding\n",
        "3. create function below `createVocabTxtFiles()` cell which visualizes top key-value pairs in `vocabFreqThreshold` text files\n",
        "4. uncomment and use model training code cell at the end of the notebook"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preparing Model Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data generator, intended to be used in a call to model.fit_generator()\n",
        "def dataGenerator(imgToCaptions, imgToFeatures, wordToIdx, vocabSize, maxCaptionLength, imgsBatchSize):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    n = 0\n",
        "    # loop forever over images\n",
        "    while True:\n",
        "        for imgId, captions in imgToCaptions.items():\n",
        "            n += 1\n",
        "            imgFeatures = imgToFeatures[imgId+'.jpg'] # retrieve the image's feature vector\n",
        "            for caption in captions:\n",
        "                seq = [wordToIdx[word] for word in caption.split(' ') if word in wordToIdx] # encode the caption into a sequence of numbers instead of words\n",
        "                for i in range(1, len(seq)): # split one sequence into multiple X, y pairs\n",
        "                    inSeq, outSeq = seq[:i], seq[i] # split into input and output pair\n",
        "                    inSeq = preprocessing.sequence.pad_sequences([inSeq], maxlen=maxCaptionLength, padding=\"pre\")[0] # pad input sequence\n",
        "                    outSeq = to_categorical([outSeq], num_classes=vocabSize)[0] # (one-hot) encodes the output sequence (note: to_categorical() is a keras-related function)\n",
        "                    X1.append(imgFeatures) # store the values\n",
        "                    X2.append(inSeq)\n",
        "                    y.append(outSeq)\n",
        "            # yield the batch data\n",
        "            if n == imgsBatchSize:\n",
        "                yield [[np.array(X1), np.array(X2)], np.array(y)] # \"yield\" saves the function's state, returns [[..]], then continues function from that statement when function is called again\n",
        "                X1, X2, y = list(), list(), list()\n",
        "                n = 0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Explanation of inner-most for loop above:\n",
        "\n",
        "<img src=\"project_media/xi_as_words.png\" width=\"600\" />\n",
        "\n",
        "However, since we're using `wordToIdx` mapping, the table above will be:\n",
        "\n",
        "<img src=\"project_media/xi_as_idxss.png\" width=\"600\" />\n",
        "\n",
        "Note 1: the table above is for the case of `post` padding. However, we'll assume `pre` padding, as it is [generally advised](https://stackoverflow.com/questions/46298793/how-does-choosing-between-pre-and-post-zero-padding-of-sequences-impact-results#:~:text=I%20always%20recommend%20using%20pre%2Dpadding%20over%20post%2Dpadding%2C%20even%20for%20CNNs%2C%20unless%20the%20problem%20specifically%20requires%20post%2Dpadding.)\n",
        "\n",
        "Note 2: under the hood, `target word` is one-hot encoded representation of the numerical values displayed in the table above; this is because one-hot encoding the target word allows us to represent this probability distribution as a vector of probabilities over the entire vocabulary, which can be directly compared to the predicted probability distribution output by the neural network  \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pipelining Preprocessing Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocessing_pipeline(freq_threshold, model_for_feature_extraction, load_features=False):\n",
        "\n",
        "    filename = \"dataset/Flicker8k_TextFiles/Flickr8k.token.txt\"\n",
        "    with open(filename, 'r') as f:\n",
        "        doc = f.read()\n",
        "    lines = doc.split('\\n')\n",
        "\n",
        "    # getting these captions in a dictionary; \n",
        "    # where the key is the image's name (without .jpg) and the value is a list of 5 captions\n",
        "    imgToCaptions = dict()\n",
        "    for line in lines:\n",
        "        idAndCaption = re.split(\"\\..+\\t\", line)\n",
        "        if len(idAndCaption) < 2:\n",
        "            continue\n",
        "        imgId, caption = idAndCaption\n",
        "        if imgId not in imgToCaptions:\n",
        "            imgToCaptions[imgId] = list()\n",
        "        imgToCaptions[imgId].append(caption)\n",
        "\n",
        "    freqThreshold = freq_threshold\n",
        "    vocab, vocabWordFreq, vocabWordFreqRemoved = createVocab(imgToCaptions, freqThreshold)\n",
        "    # createVocabTxtFiles(vocabWordFreq, vocabWordFreqRemoved, filePrefix=f\"vocabFreqThreshold{freqThreshold}\")\n",
        "    idxToWord, wordToIdx = mapIdxAndWord(vocab)\n",
        "    vocabSize = len(idxToWord) + 1 # one for appended 0's; represents \"startseq\" (explained in \"Preparing Model Generator\" section)\n",
        "\n",
        "    trainImgsIds = getImgsIdsList('dataset/Flicker8k_TextFiles/Flickr_8k.trainImages.txt')\n",
        "    valImgsIds = getImgsIdsList('dataset/Flicker8k_TextFiles/Flickr_8k.devImages.txt')\n",
        "    testImgsIds = getImgsIdsList('dataset/Flicker8k_TextFiles/Flickr_8k.testImages.txt')\n",
        "\n",
        "    trainImgToCaptions = dict()\n",
        "    valImgToCaptions = dict()\n",
        "    testImgToCaptions = dict()\n",
        "    for imgId in trainImgsIds:\n",
        "        trainImgToCaptions[imgId] = ['startseq ' + desc + ' endseq' for desc in imgToCaptions[imgId]]\n",
        "    for imgId in valImgsIds:\n",
        "        valImgToCaptions[imgId] = ['startseq ' + desc + ' endseq' for desc in imgToCaptions[imgId]]\n",
        "    for imgId in testImgsIds:\n",
        "        testImgToCaptions[imgId] = ['startseq ' + desc + ' endseq' for desc in imgToCaptions[imgId]]\n",
        "\n",
        "    datasetImgsBasePath = 'dataset/Flicker8k_Dataset/'\n",
        "    fullImgsPath = glob(datasetImgsBasePath + '*.jpg')\n",
        "    trainImgsPaths = [joinPaths(datasetImgsBasePath, imgId)+'.jpg' for imgId in trainImgToCaptions.keys()]\n",
        "    valImgsPaths = [joinPaths(datasetImgsBasePath, imgId)+'.jpg' for imgId in valImgToCaptions.keys()]\n",
        "    testImgsPaths = [joinPaths(datasetImgsBasePath, imgId)+'.jpg' for imgId in testImgToCaptions.keys()]\n",
        "\n",
        "    if load_features:\n",
        "        trainImgToFeatures = pklLoad('dataset/pickles/trainImgToFeatures.pickle')\n",
        "        valImgToFeatures = pklLoad('dataset/pickles/valImgToFeatures.pickle')\n",
        "        testImgToFeatures = pklLoad('dataset/pickles/testImgToFeatures.pickle')   \n",
        "    else: \n",
        "        trainImgToFeatures = encodeImgToFeatures(trainImgsPaths, model_for_feature_extraction) # shape of each encoded image: (2048,); returned by Google's Inception Model\n",
        "        valImgToFeatures = encodeImgToFeatures(valImgsPaths, model_for_feature_extraction)\n",
        "        testImgToFeatures = encodeImgToFeatures(testImgsPaths, model_for_feature_extraction)\n",
        "        # pklSave(trainImgToFeatures, 'dataset/pickles/trainImgToFeatures.pickle')\n",
        "    # pklSave(valImgToFeatures, 'dataset/pickles/valImgToFeatures.pickle')\n",
        "    # pklSave(testImgToFeatures, 'dataset/pickles/testImgToFeatures.pickle')\n",
        "\n",
        "    # determine the maximum sequence length\n",
        "    maxCapLen = maxCaptionLength(trainImgToCaptions)\n",
        "\n",
        "    return (imgToCaptions, vocab, vocabWordFreq, vocabWordFreqRemoved, \n",
        "            idxToWord, wordToIdx, vocabSize, maxCapLen,\n",
        "            trainImgsIds, valImgsIds, testImgsIds, \n",
        "            trainImgToFeatures, valImgToFeatures, testImgToFeatures, \n",
        "            trainImgToCaptions, valImgToCaptions, testImgToCaptions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Word Embedding"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Glove Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original vocabulary (i.e., unique words) size: 9630\n",
            "Vocabulary size after removing less frequent words (< 5 words): 3107\n"
          ]
        }
      ],
      "source": [
        "# hyperparameters to tune\n",
        "freq_threshold = 5\n",
        "model = InceptionV3(weights='imagenet') # getting the InceptionV3 model trained on imagenet data\n",
        "model_for_feature_extraction = Model(model.input, model.layers[-2].output) # removing the last layer (output softmax layer)\n",
        "imgs_batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "(imgToCaptions, vocab, vocabWordFreq, vocabWordFreqRemoved, \n",
        "idxToWord, wordToIdx, vocabSize, maxCapLen,\n",
        "trainImgsIds, valImgsIds, testImgsIds, \n",
        "trainImgToFeatures, valImgToFeatures, testImgToFeatures, \n",
        "trainImgToCaptions, valImgToCaptions, testImgToCaptions) = preprocessing_pipeline(freq_threshold, model_for_feature_extraction, load_features=False) # set load_features=True if you won't change \"weights\" argument of \"model\" variable to avoid bottleneck of using Google's Inception Model\n",
        "\n",
        "train_datagen = dataGenerator(trainImgToCaptions, trainImgToFeatures, wordToIdx, vocabSize, maxCapLen, imgs_batch_size)\n",
        "val_datagen = dataGenerator(valImgToCaptions, valImgToFeatures, wordToIdx, vocabSize, maxCapLen, imgs_batch_size)\n",
        "test_datagen = dataGenerator(testImgToCaptions, testImgToFeatures, wordToIdx, vocabSize, maxCapLen, imgs_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "steps = len(trainImgToCaptions) // imgs_batch_size\n",
        "for i in range(1, epochs+1):\n",
        "    train_datagen = dataGenerator(trainImgToCaptions, trainImgToFeatures, wordToIdx, vocabSize, maxCapLen, imgs_batch_size)\n",
        "    model.fit(train_datagen, epochs=epochs, steps_per_epoch=steps, verbose=2)\n",
        "    model.save(f'models/changingEpochs/modelWith{str(i)}Epochs.h5') # change file name to include other hyperparameters as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# to do (phase 2):\n",
        "# code for embedding\n",
        "# code for model architecture\n",
        "# code for model training; provided here: vvv\n",
        "\n",
        "# epochs = 10\n",
        "# numImgsPerBatch = 3\n",
        "# steps = len(trainImgToCaptions)//numImgsPerBatch\n",
        "# for i in range(1, epochs+1):\n",
        "#     generator = dataGenerator(trainImgToCaptions, trainImgToFeatures, wordToIdx, maxCapLen, numImgsPerBatch)\n",
        "#     model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "#     model.save(f'models/changingEpochs/modelWith{str(i)}Epochs.h5')\n",
        "\n",
        "# code for validating/testing (inference)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c7cdde67bd1df58aee8e05336b4c9e41dfb37650659bf34d4c10242d0898ac5a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
