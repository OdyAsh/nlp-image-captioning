{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Side note: all the cells until the `Pipelining Preprocessing Steps` are similar to the cells in `...phase_1.ipynb` file, as we've worked on preprocessing steps to be used in phase 2 when writing the code for phase 1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cells for Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iL10kckWVsh1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "runningFromColab = False\n",
        "if 'CGROUP_MEMORY_EVENTS' in os.environ and 'colab' in os.environ['CGROUP_MEMORY_EVENTS']:\n",
        "  runningFromColab = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTIJGz2iHMO2",
        "outputId": "326dfca4-9445-4816-911e-b765a922e293"
      },
      "outputs": [],
      "source": [
        "if runningFromColab:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of0Zg47kK5c6",
        "outputId": "63bb8d76-b03d-4416-d307-7316ca5bb0fc"
      },
      "outputs": [],
      "source": [
        "if runningFromColab:\n",
        "  %cd /content/drive/MyDrive/ColabProjects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzrpiAjyLPrb",
        "outputId": "7ee9cf1d-4a35-4374-fbf3-d226e19ffc11"
      },
      "outputs": [],
      "source": [
        "if runningFromColab:\n",
        "  !git clone https://github.com/OdyAsh/nlp-image-captioning.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgJWSx0ENHfi",
        "outputId": "b932304f-617a-4fa5-b586-011925faa17b"
      },
      "outputs": [],
      "source": [
        "if runningFromColab:\n",
        "  %cd /content/drive/MyDrive/ColabProjects/nlp-image-captioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oggUYhWqPZMX",
        "outputId": "1e4791d0-9831-47ab-a2ed-5cd18121ae74"
      },
      "outputs": [],
      "source": [
        "if runningFromColab:\n",
        "  !git pull\n",
        "  # if it DOES NOT say \"Already up to date.\", then you need to close this notebook file (i.e., the browser tab) and open it again for it to change "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLTA8a3-Q7ph",
        "outputId": "bef4bb2c-d71a-49bb-f060-0e352def13ab"
      },
      "outputs": [],
      "source": [
        "# if runningFromColab:\n",
        "#   try:\n",
        "#     import condacolab\n",
        "#     condacolab.install()\n",
        "#   except:\n",
        "#     !pip install -q condacolab\n",
        "#     import condacolab\n",
        "#     condacolab.install()\n",
        "#     # now restart the kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW1oaGvGMsvN",
        "outputId": "8722ca24-ba6c-42ca-8be3-d8c103cd3897"
      },
      "outputs": [],
      "source": [
        "# if runningFromColab:\n",
        "#   !conda env create -f environment.yml\n",
        "#   # !conda update conda -y -q\n",
        "#   # !source /usr/local/etc/profile.d/conda.sh\n",
        "#   # !conda init \n",
        "#   # !conda install -n root _license -y -q\n",
        "#   # !source activate myenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7zUUVjcRLxjd"
      },
      "outputs": [],
      "source": [
        "# if runningFromColab:\n",
        "#   import sys\n",
        "#   sys.path.insert(0, '/usr/local/bin/conda')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gjH1mCsmHKZR"
      },
      "source": [
        "# Imports & Global Functions/Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "from glob import glob\n",
        "from time import time\n",
        "import os\n",
        "import pickle\n",
        "import regex as re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
        "#                          Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
        "# from tensorflow.keras.layers import Bidirectional\n",
        "# from tensorflow.keras.layers import Add # merge.add\n",
        "from tensorflow.keras.applications import inception_v3 # inception_v3.preprocess_input\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras import preprocessing # preprocessing.image, preprocessing.sequence, preprocessing.text.Tokenizer, preprocessing.sequence.pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import Input, layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Shehab\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pklSave(contentToBeSaved, fullPath):\n",
        "    with open(fullPath, 'wb') as f:\n",
        "        pickle.dump(contentToBeSaved, f)\n",
        "\n",
        "def pklLoad(fullPath):\n",
        "    with open(fullPath, 'rb') as f:\n",
        "        content = pickle.load(f)\n",
        "    return content\n",
        "\n",
        "def pklForceLoad(path, dtype = 'dict'):\n",
        "    try:\n",
        "        content = pklLoad(path)\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        if dtype == 'list':\n",
        "            pklSave([], path)\n",
        "            return []\n",
        "        else:\n",
        "            pklSave({}, path)\n",
        "            return {}\n",
        "\n",
        "# more about naming standards for path components here: https://stackoverflow.com/questions/2235173/what-is-the-naming-standard-for-path-components\n",
        "def joinPaths(baseDirectory, relativePath):\n",
        "    return os.path.normpath(os.path.join(baseDirectory, relativePath))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8091"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datasetImgsBasePath = 'dataset/Flicker8k_Dataset/'\n",
        "fullImgsPath = glob(datasetImgsBasePath + '*.jpg')\n",
        "fullImgsPaths = [os.path.normpath(path) for path in fullImgsPath]\n",
        "len(fullImgsPaths)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Collection\n",
        "The dataset is obtained from [here](https://forms.illinois.edu/sec/1713398)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "first image's captions:\n",
            "['1000268201_693b08cb0e.jpg#0\\tA child in a pink dress is climbing up a set of '\n",
            " 'stairs in an entry way .',\n",
            " '1000268201_693b08cb0e.jpg#1\\tA girl going into a wooden building .',\n",
            " '1000268201_693b08cb0e.jpg#2\\tA little girl climbing into a wooden playhouse '\n",
            " '.',\n",
            " '1000268201_693b08cb0e.jpg#3\\tA little girl climbing the stairs to her '\n",
            " 'playhouse .',\n",
            " '1000268201_693b08cb0e.jpg#4\\tA little girl in a pink dress going into a '\n",
            " 'wooden cabin .']\n",
            "\n",
            "second image's captions:\n",
            "['1001773457_577c3a7d70.jpg#0\\tA black dog and a spotted dog are fighting',\n",
            " '1001773457_577c3a7d70.jpg#1\\tA black dog and a tri-colored dog playing with '\n",
            " 'each other on the road .',\n",
            " '1001773457_577c3a7d70.jpg#2\\tA black dog and a white dog with brown spots '\n",
            " 'are staring at each other in the street .',\n",
            " '1001773457_577c3a7d70.jpg#3\\tTwo dogs of different breeds looking at each '\n",
            " 'other on the road .',\n",
            " '1001773457_577c3a7d70.jpg#4\\tTwo dogs on pavement moving toward each other .']\n",
            "\n",
            "and so forth...\n"
          ]
        }
      ],
      "source": [
        "# checking the 5 captions per image\n",
        "filename = \"dataset/Flicker8k_TextFiles/Flickr8k.token.txt\"\n",
        "with open(filename, 'r') as f:\n",
        "    doc = f.read()\n",
        "lines = doc.split('\\n')\n",
        "print('first image\\'s captions:')\n",
        "pprint(lines[:5])\n",
        "print('\\nsecond image\\'s captions:')\n",
        "pprint(lines[5:10])\n",
        "print('\\nand so forth...')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The captions above are for these two images:\n",
        "\n",
        "<img src=\"project_media/1000268201_693b08cb0e.jpg\" width=\"100\" />\n",
        "\n",
        "<img src=\"project_media/1001773457_577c3a7d70.jpg\" width=\"150\" />"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Cleaning\n",
        "Includes:\n",
        "* `imgToCaptions` dictionary\n",
        "* `cleanCaptions()` to remove stopwords/punctuations\n",
        "* `createVocab()` to limit vocab size based on word frequency\n",
        "* `train`, `val`, and `test` `ImgToCaptions` which prepends `startseq` and appends `endseq`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['A child in a pink dress is climbing up a set of stairs in an entry way .',\n",
              " 'A girl going into a wooden building .',\n",
              " 'A little girl climbing into a wooden playhouse .',\n",
              " 'A little girl climbing the stairs to her playhouse .',\n",
              " 'A little girl in a pink dress going into a wooden cabin .']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# getting these captions in a dictionary; \n",
        "# where the key is the image's name (without .jpg) and the value is a list of 5 captions\n",
        "\n",
        "imgToCaptions = dict()\n",
        "for line in lines:\n",
        "    idAndCaption = re.split(\"\\..+\\t\", line)\n",
        "    if len(idAndCaption) < 2:\n",
        "        continue\n",
        "    imgId, caption = idAndCaption\n",
        "    if imgId not in imgToCaptions:\n",
        "        imgToCaptions[imgId] = list()\n",
        "    imgToCaptions[imgId].append(caption)\n",
        "    \n",
        "imgToCaptions['1000268201_693b08cb0e']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['child in pink dress is climbing up set of stairs in entry way',\n",
              " 'girl going into wooden building',\n",
              " 'little girl climbing into wooden playhouse',\n",
              " 'little girl climbing stairs to her playhouse',\n",
              " 'little girl in pink dress going into wooden cabin']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# removing punctuation using maketrans (i.e., translation table)\n",
        "# more about maketrans method: https://www.w3schools.com/python/ref_string_maketrans.asp#:~:text=The%20third%20parameter%20in%20the%20mapping%20table%20describes%20characters%20that%20you%20want%20to%20remove%20from%20the%20string%3A\n",
        "\n",
        "# to do: ASK Dr: We've removed few stopwords in order for generated caption to make sense, is this logical?\n",
        "#                We've also removed numbers\n",
        "\n",
        "def cleanCaptions(imgToCaptions, levelOfStopwordsPresence=1):\n",
        "    table = str.maketrans('', '', string.punctuation) # third argument: removes any character in this list: '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "    for key, descList in imgToCaptions.items():\n",
        "        # when this for loop is done, all 5 captions of an image will be cleaned\n",
        "        for i in range(len(descList)):\n",
        "            desc = descList[i]\n",
        "            desc = desc.split(' ')\n",
        "            desc = [word.lower() for word in desc]\n",
        "            desc = [word.translate(table) for word in desc] # remove punctuation from each token\n",
        "            stopswordsToRemove = []\n",
        "            if levelOfStopwordsPresence == 1:\n",
        "                stopswordsToRemove = ['a', 'an', 'the']\n",
        "            elif levelOfStopwordsPresence >= 2:\n",
        "                stopswordsToRemove = set(stopwords.words('english'))\n",
        "            desc = [word for word in desc if word not in stopswordsToRemove]\n",
        "            desc = [word for word in desc if word.isalpha()] # remove tokens with numbers in them\n",
        "            descList[i] =  ' '.join(desc) # store as string\n",
        "\n",
        "# cleanCaptions(imgToCaptions, levelOfStopwordsPresence=1)\n",
        "# pklSave(imgToCaptions, 'dataset/pickles/imgToCaptionsSWKept.pickle')\n",
        "imgToCaptions = pklLoad('dataset/pickles/imgToCaptionsSWKept.pickle')\n",
        "imgToCaptions['1000268201_693b08cb0e']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "example with stopwords removed:\n",
        "<br><br>\n",
        "'little girl climbing stairs playhouse',\n",
        "\n",
        "<br>\n",
        "example with only ['a', 'an', 'the'] removed:\n",
        "<br><br>\n",
        "'little girl climbing stairs to her playhouse',\n",
        "<br><br>\n",
        "from the lack of context seen above, we've decided to keep the rest of the stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original vocabulary (i.e., unique words) size: 8366\n",
            "Vocabulary size after removing less frequent words (< 5 words): 2945\n"
          ]
        }
      ],
      "source": [
        "# creating vocab of unique words (where each word occured at least freqThreshold number of times)\n",
        "def createVocab(imgToCaptions, freqThreshold = 10):\n",
        "    vocab = set()\n",
        "    for key in imgToCaptions.keys():\n",
        "        [vocab.update(desc.split()) for desc in imgToCaptions[key]]\n",
        "    print(f'Original vocabulary (i.e., unique words) size: {len(vocab)}')\n",
        "\n",
        "    # keeping words that appear at least freqThrehold number of times\n",
        "    vocabWordFreq = {key: 0 for key in vocab}\n",
        "    for key, descs in imgToCaptions.items():\n",
        "        for desc in descs:\n",
        "            descList = desc.split(' ')\n",
        "            for word in descList:\n",
        "                if word != '':\n",
        "                    vocabWordFreq[word] += 1\n",
        "    \n",
        "    vocab = set()\n",
        "    i = 0\n",
        "    for word, freq in vocabWordFreq.items():\n",
        "        if freq >= freqThreshold:\n",
        "            i += 1\n",
        "            vocab.add(word)\n",
        "            vocabWordFreq[word] = freq\n",
        "    print(f'Vocabulary size after removing less frequent words (< {freqThreshold} words): {len(vocab)}')\n",
        "\n",
        "    vocabWordFreqRemoved = {word: freq for word, freq in vocabWordFreq.items() if word not in vocabWordFreq}\n",
        "\n",
        "    return vocab, vocabWordFreq, vocabWordFreqRemoved\n",
        "\n",
        "def createVocabTxtFiles(vocabWordFreq, vocabWordFreqRemoved, filePrefix=\"vocabFreqThreshold\"):\n",
        "    with open(f'dataset/{filePrefix}.txt', 'w') as f:\n",
        "        f.write(str(dict(sorted(vocabWordFreq.items(), key=lambda x: x[1], reverse=True))))\n",
        "    with open(f'dataset/{filePrefix}Removed.txt', 'w') as f:\n",
        "        f.write(str(dict(sorted(vocabWordFreqRemoved.items(), key=lambda x: x[1], reverse=True))))\n",
        "\n",
        "freqThreshold = 5\n",
        "vocab, vocabWordFreq, vocabWordFreqRemoved = createVocab(imgToCaptions, freqThreshold)\n",
        "createVocabTxtFiles(vocabWordFreq, vocabWordFreqRemoved, filePrefix=f\"vocabFreqThreshold{freqThreshold}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Dataset: 6000\n",
            "Validation Dataset: 1000\n",
            "Test Dataset: 1000\n"
          ]
        }
      ],
      "source": [
        "# function to get filenames of images from a text file (without the extension)\n",
        "def getImgsIdsList(txtPath):\n",
        "    with open(txtPath, 'r') as f:\n",
        "        doc = f.read()\n",
        "    ImgsIds = []\n",
        "    for line in doc.split('\\n'):\n",
        "        imgId = line.split('.')[0]\n",
        "        ImgsIds.append(imgId)\n",
        "    ImgsIds = [id for id in ImgsIds if id != '']\n",
        "    return ImgsIds\n",
        "\n",
        "trainImgsIds = getImgsIdsList('dataset/Flicker8k_TextFiles/Flickr_8k.trainImages.txt')\n",
        "valImgsIds = getImgsIdsList('dataset/Flicker8k_TextFiles/Flickr_8k.devImages.txt')\n",
        "testImgsIds = getImgsIdsList('dataset/Flicker8k_TextFiles/Flickr_8k.testImages.txt')\n",
        "print(f'Train Dataset: {len(trainImgsIds)}')\n",
        "print(f'Validation Dataset: {len(valImgsIds)}')\n",
        "print(f'Test Dataset: {len(testImgsIds)}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the code below, we use `startseq` and `endseq` for the following reasons:\n",
        "* startseq : Will indicate the start of the caption generation process\n",
        "* endseq : to stop predicting words as soon as it appears"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "images in training set: 6000\n",
            "\n",
            "images in validation set: 1000\n",
            "\n",
            "images in testing set: 1000\n",
            "\n",
            "example from training set:\n",
            "['startseq black dog is running after white dog in snow endseq',\n",
            " 'startseq black dog chasing brown dog through snow endseq',\n",
            " 'startseq two dogs chase each other across snowy ground endseq',\n",
            " 'startseq two dogs play together in snow endseq',\n",
            " 'startseq two dogs running through low lying body of water endseq']\n"
          ]
        }
      ],
      "source": [
        "trainImgToCaptions = dict()\n",
        "valImgToCaptions = dict()\n",
        "testImgToCaptions = dict()\n",
        "for imgId in trainImgsIds:\n",
        "    trainImgToCaptions[imgId] = ['startseq ' + desc + ' endseq' for desc in imgToCaptions[imgId]]\n",
        "for imgId in valImgsIds:\n",
        "    valImgToCaptions[imgId] = ['startseq ' + desc + ' endseq' for desc in imgToCaptions[imgId]]\n",
        "for imgId in testImgsIds:\n",
        "    testImgToCaptions[imgId] = ['startseq ' + desc + ' endseq' for desc in imgToCaptions[imgId]]\n",
        "print(f'images in training set: {len(trainImgToCaptions)}\\n')\n",
        "print(f'images in validation set: {len(valImgToCaptions)}\\n')\n",
        "print(f'images in testing set: {len(testImgToCaptions)}\\n')\n",
        "print('example from training set:')\n",
        "pprint(trainImgToCaptions['2513260012_03d33305cf'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preparing File Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6000, 1000, 1000)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainImgsPaths = [joinPaths(datasetImgsBasePath, imgId)+'.jpg' for imgId in trainImgToCaptions.keys()]\n",
        "valImgsPaths = [joinPaths(datasetImgsBasePath, imgId)+'.jpg' for imgId in valImgToCaptions.keys()]\n",
        "testImgsPaths = [joinPaths(datasetImgsBasePath, imgId)+'.jpg' for imgId in testImgToCaptions.keys()]\n",
        "len(trainImgsPaths), len(valImgsPaths), len(testImgsPaths)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Pre-Processing\n",
        "Includes:\n",
        "* Pre-Processing Images\n",
        "* Pre-Processing Captions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre-Processing Images\n",
        "Includes:\n",
        "* Loading Google's `InceptionV3` model\n",
        "* Preprocessing the image\n",
        "* Encoding the image by inputting it to `InceptionV3` to get a `2048` feature vector of the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 1000) dtype=float32 (created by layer 'predictions')>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# getting the feature vector of each image using the InceptionV3 CNN model created by Google Research\n",
        "inception_model = InceptionV3(weights='imagenet') # getting the InceptionV3 model trained on imagenet data\n",
        "inception_model.layers[-1].output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 2048) dtype=float32 (created by layer 'avg_pool')>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "modelForFeatureExtraction = Model(inception_model.input, inception_model.layers[-2].output) # removing the last layer (output softmax layer)\n",
        "modelForFeatureExtraction.layers[-1].output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to preprocess the input image\n",
        "def preprocess(imgPath):\n",
        "    pilImg = preprocessing.image.load_img(imgPath, target_size=(299, 299)) # Convert all the images to size 299x299 as expected by the inception v3 model\n",
        "    x = preprocessing.image.img_to_array(pilImg) # Convert PIL image to numpy array of 3-dimensions\n",
        "    x = np.expand_dims(x, axis=0) # Add one more dimension; from (299, 299, 3) to (1, 299, 299, 3)\n",
        "    x = inception_v3.preprocess_input(x) # takes in (batch_size, height, width, channels), returns same dimensions, but does some preprocessing operations, like scaling values to be from -1 to 1\n",
        "    return x\n",
        "\n",
        "# function to encode a given image (from its path) into a vector of size (2048, )\n",
        "def encode(imgPath, modelForFeatureExtraction):\n",
        "    imgPath = preprocess(imgPath) # preprocess the image\n",
        "    featureVec = modelForFeatureExtraction.predict(imgPath) # Get the encoding vector for the image\n",
        "    featureVec = np.reshape(featureVec, featureVec.shape[1]) # reshape from (1, 2048) to (2048, )\n",
        "    return featureVec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6000, 1000, 1000, (2048,))"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Call the funtion to encode all the train images (dictionary where an image id --> feature vector of length 2048)\n",
        "# This will take a while on CPU - Execute this only once (took around 13 minutes on my high-end laptop)\n",
        "def encodeImgToFeatures(imgsPaths, modelForFeatureExtraction):\n",
        "    imgToFeatures = dict()\n",
        "    for imgPath in imgsPaths:\n",
        "        imgToFeatures[imgPath[len(datasetImgsBasePath):]] = encode(imgPath, modelForFeatureExtraction)\n",
        "    return imgToFeatures\n",
        "\n",
        "# trainImgToFeatures = encodeImgToFeatures(trainImgsPaths, modelForFeatureExtraction)\n",
        "# valImgToFeatures = encodeImgToFeatures(valImgsPaths, modelForFeatureExtraction)\n",
        "# testImgToFeatures = encodeImgToFeatures(testImgsPaths, modelForFeatureExtraction)\n",
        "# pklSave(trainImgToFeatures, 'dataset/pickles/trainImgToFeatures.pickle')\n",
        "# pklSave(valImgToFeatures, 'dataset/pickles/valImgToFeatures.pickle')\n",
        "# pklSave(testImgToFeatures, 'dataset/pickles/testImgToFeatures.pickle')\n",
        "trainImgToFeatures = pklLoad('dataset/pickles/trainImgToFeatures.pickle')\n",
        "valImgToFeatures = pklLoad('dataset/pickles/valImgToFeatures.pickle')\n",
        "testImgToFeatures = pklLoad('dataset/pickles/testImgToFeatures.pickle')\n",
        "len(trainImgToFeatures), len(valImgToFeatures), len(testImgToFeatures), trainImgToFeatures['2513260012_03d33305cf.jpg'].shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre-Processing Captions\n",
        "Includes:\n",
        "* `mapIdxAndWord()` to map indices to words and vice versa, where the words are obtained from `createVocab()`\n",
        "* `maxCaptionLength()` to get the caption with the most amount of words, to be used later to pad input sequences <br> (explained in `Preparing Model Generator` section)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original vocabulary (i.e., unique words) size: 8366\n",
            "Vocabulary size after removing less frequent words (< 5 words): 2945\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2946"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# creating two dictionaries: word to index, and index to word\n",
        "\n",
        "def mapIdxAndWord(vocab):\n",
        "    idxToWord = {}\n",
        "    wordToIdx = {}\n",
        "    idx = 1\n",
        "    for word in vocab:\n",
        "        wordToIdx[word] = idx\n",
        "        idxToWord[idx] = word\n",
        "        idx += 1\n",
        "    return idxToWord, wordToIdx\n",
        "\n",
        "vocab, _, _ = createVocab(imgToCaptions, freqThreshold=5)\n",
        "idxToWord, wordToIdx = mapIdxAndWord(vocab)\n",
        "vocabSize = len(idxToWord) + 1 # one for appended 0's; represents \"startseq\" (explained in \"Preparing Model Generator\" section)\n",
        "vocabSize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Description Length: 32\n"
          ]
        }
      ],
      "source": [
        "# getting the length of the longest caption; as we will later need to encode each word into a fixed sized vector\n",
        "\n",
        "# convert a dictionary of clean captions to a list of captions\n",
        "def toCaptionsList(ImgToCaptions):\n",
        "\tcaptionsList = list()\n",
        "\tfor imgId in ImgToCaptions.keys():\n",
        "\t\t[captionsList.append(caption) for caption in ImgToCaptions[imgId]]\n",
        "\treturn captionsList\n",
        "\n",
        "# calculate the length of the description with the most words\n",
        "def maxCaptionLength(ImgToCaptions):\n",
        "    captions = toCaptionsList(ImgToCaptions)\n",
        "    captionsLengths = [len(caption.split()) for caption in captions]\n",
        "    return max(captionsLengths)\n",
        "\n",
        "# determine the maximum sequence length\n",
        "maxCapLen = maxCaptionLength(trainImgToCaptions)\n",
        "print(f'Description Length: {maxCapLen}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "maxCaptionLength(testImgToCaptions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Word Embedding"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Glove Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "# run the commented code lines below once to create word_to_glove_embedding dictionary, then load it using pklLoad()\n",
        "\n",
        "# Load Glove vectors\n",
        "glove_dir = './dataset/glove_word_embeddings/'\n",
        "# word_to_glove_embedding = {} # empty dictionary\n",
        "# with open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding=\"utf-8\") as f:\n",
        "#     for line in f:\n",
        "#         word_and_embedding_of_word = line.split()\n",
        "#         word = word_and_embedding_of_word[0]\n",
        "#         coefs = np.asarray(word_and_embedding_of_word[1:], dtype='float32')\n",
        "#         word_to_glove_embedding[word] = coefs\n",
        "# pklSave(word_to_glove_embedding, './dataset/pickles/word_to_glove_embedding.pickle')\n",
        "\n",
        "word_to_glove_embedding = pklLoad('./dataset/pickles/word_to_glove_embedding.pickle')\n",
        "\n",
        "print('Found %s word vectors.' % len(word_to_glove_embedding))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_embedding_matrix(vocabSize, embedding_dim=200):\n",
        "    # Get embedding_dim dense vector for each word in our vocabulary\n",
        "    embedding_matrix = np.zeros((vocabSize, embedding_dim))\n",
        "\n",
        "    words_with_no_embedding = []\n",
        "    for word, i in wordToIdx.items():\n",
        "        #if i < max_words:\n",
        "        embedding_vector = word_to_glove_embedding.get(word)\n",
        "        if embedding_vector is None:\n",
        "            # Words not found in the embedding index will be all zeros\n",
        "            words_with_no_embedding.append(word)\n",
        "            continue\n",
        "        embedding_matrix[i] = embedding_vector[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix, words_with_no_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of words with no embeddings: 15\n",
            "which are:\n",
            "['surfboarder', 'plushie', 'corndogs', 'waterskier', 'wakeboards', 'dalmation', 'inground', 'rollerblader', 'bicycler', 'floaties', 'wakeboarder', 'parasails', 'outstreached', 'windsurfs', 'somthing']\n",
            "embedding matrix dimensions (vocab size x embedding dimensions):\n",
            "(2946, 200)\n"
          ]
        }
      ],
      "source": [
        "# for debugging: the code above will be re-written in preprocessing_pipeline()\n",
        "\n",
        "#hyperparameter:\n",
        "embedding_dim = 200 # Note: 200 is the maximum number of dimensions specified in glove.6B.200d.txt\n",
        "\n",
        "embedding_matrix, words_with_no_embedding = get_embedding_matrix(vocabSize, embedding_dim)\n",
        "print('number of words with no embeddings:', len(words_with_no_embedding))\n",
        "print('which are:')\n",
        "print(words_with_no_embedding)\n",
        "print('embedding matrix dimensions (vocab size x embedding dimensions):')\n",
        "print(embedding_matrix.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pipelining Preprocessing Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocessing_pipeline(freq_threshold, model_for_feature_extraction, embedding_dim=200, load_features=False): # Note: in embedding_dim, 200 is the maximum number of dimensions specified in glove.6B.200d.txt\n",
        "\n",
        "    filename = \"dataset/Flicker8k_TextFiles/Flickr8k.token.txt\"\n",
        "    with open(filename, 'r') as f:\n",
        "        doc = f.read()\n",
        "    lines = doc.split('\\n')\n",
        "\n",
        "    # getting these captions in a dictionary; \n",
        "    # where the key is the image's name (without .jpg) and the value is a list of 5 captions\n",
        "    imgToCaptions = dict()\n",
        "    for line in lines:\n",
        "        idAndCaption = re.split(\"\\..+\\t\", line)\n",
        "        if len(idAndCaption) < 2:\n",
        "            continue\n",
        "        imgId, caption = idAndCaption\n",
        "        if imgId not in imgToCaptions:\n",
        "            imgToCaptions[imgId] = list()\n",
        "        imgToCaptions[imgId].append(caption)\n",
        "\n",
        "    freqThreshold = freq_threshold\n",
        "    vocab, vocabWordFreq, vocabWordFreqRemoved = createVocab(imgToCaptions, freqThreshold)\n",
        "    # createVocabTxtFiles(vocabWordFreq, vocabWordFreqRemoved, filePrefix=f\"vocabFreqThreshold{freqThreshold}\")\n",
        "    idxToWord, wordToIdx = mapIdxAndWord(vocab)\n",
        "    vocabSize = len(idxToWord) + 1 # one for appended 0's; represents \"startseq\" (explained in \"Preparing Model Generator\" section)\n",
        "\n",
        "    trainImgsIds = getImgsIdsList('dataset/Flicker8k_TextFiles/Flickr_8k.trainImages.txt')\n",
        "    valImgsIds = getImgsIdsList('dataset/Flicker8k_TextFiles/Flickr_8k.devImages.txt')\n",
        "    testImgsIds = getImgsIdsList('dataset/Flicker8k_TextFiles/Flickr_8k.testImages.txt')\n",
        "\n",
        "    trainImgToCaptions = dict()\n",
        "    valImgToCaptions = dict()\n",
        "    testImgToCaptions = dict()\n",
        "    for imgId in trainImgsIds:\n",
        "        trainImgToCaptions[imgId] = ['startseq ' + desc + ' endseq' for desc in imgToCaptions[imgId]]\n",
        "    for imgId in valImgsIds:\n",
        "        valImgToCaptions[imgId] = ['startseq ' + desc + ' endseq' for desc in imgToCaptions[imgId]]\n",
        "    for imgId in testImgsIds:\n",
        "        testImgToCaptions[imgId] = ['startseq ' + desc + ' endseq' for desc in imgToCaptions[imgId]]\n",
        "\n",
        "    datasetImgsBasePath = 'dataset/Flicker8k_Dataset/'\n",
        "    fullImgsPath = glob(datasetImgsBasePath + '*.jpg')\n",
        "    trainImgsPaths = [joinPaths(datasetImgsBasePath, imgId)+'.jpg' for imgId in trainImgToCaptions.keys()]\n",
        "    valImgsPaths = [joinPaths(datasetImgsBasePath, imgId)+'.jpg' for imgId in valImgToCaptions.keys()]\n",
        "    testImgsPaths = [joinPaths(datasetImgsBasePath, imgId)+'.jpg' for imgId in testImgToCaptions.keys()]\n",
        "\n",
        "    if load_features:\n",
        "        trainImgToFeatures = pklLoad('dataset/pickles/trainImgToFeatures.pickle')\n",
        "        valImgToFeatures = pklLoad('dataset/pickles/valImgToFeatures.pickle')\n",
        "        testImgToFeatures = pklLoad('dataset/pickles/testImgToFeatures.pickle')   \n",
        "    else: \n",
        "        trainImgToFeatures = encodeImgToFeatures(trainImgsPaths, model_for_feature_extraction) # shape of each encoded image: (2048,); returned by Google's Inception Model\n",
        "        valImgToFeatures = encodeImgToFeatures(valImgsPaths, model_for_feature_extraction)\n",
        "        testImgToFeatures = encodeImgToFeatures(testImgsPaths, model_for_feature_extraction)\n",
        "        # pklSave(trainImgToFeatures, 'dataset/pickles/trainImgToFeatures.pickle')\n",
        "    # pklSave(valImgToFeatures, 'dataset/pickles/valImgToFeatures.pickle')\n",
        "    # pklSave(testImgToFeatures, 'dataset/pickles/testImgToFeatures.pickle')\n",
        "\n",
        "    # determine the maximum sequence length\n",
        "    maxCapLen = maxCaptionLength(trainImgToCaptions)\n",
        "\n",
        "    embedding_matrix, words_with_no_embedding = get_embedding_matrix(vocabSize, embedding_dim)\n",
        "    print('number of words with no embeddings:', len(words_with_no_embedding))\n",
        "    print('which are:')\n",
        "    print(words_with_no_embedding)\n",
        "    print('embedding matrix dimensions (vocab size x embedding dimensions):')\n",
        "    print(embedding_matrix.shape)\n",
        "\n",
        "    return (imgToCaptions, vocab, vocabWordFreq, vocabWordFreqRemoved, \n",
        "            idxToWord, wordToIdx, vocabSize, maxCapLen,\n",
        "            embedding_matrix, words_with_no_embedding,\n",
        "            trainImgsIds, valImgsIds, testImgsIds, \n",
        "            trainImgToFeatures, valImgToFeatures, testImgToFeatures, \n",
        "            trainImgToCaptions, valImgToCaptions, testImgToCaptions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
        "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
        "\n",
        "def create_lstm_model(embedding_matrix, vocabSize, droput_rate = 0.5, dense_layer_units = 256, hidden_layers_activation = 'relu'): # to do: add any extra hyperparameters\n",
        "    inputs1 = Input(shape=(2048,)) # 2048 is fixed, as this is an image's feature vector size returned by Google's Inception model\n",
        "    fe1 = Dropout(droput_rate)(inputs1)\n",
        "    fe2 = Dense(dense_layer_units, activation=hidden_layers_activation)(fe1)\n",
        "    inputs2 = Input(shape=(maxCapLen,))\n",
        "    se1 = Embedding(vocabSize, embedding_dim, mask_zero=True)(inputs2)\n",
        "    se2 = Dropout(droput_rate)(se1)\n",
        "    se3 = LSTM(dense_layer_units)(se2) # to do: add any extra hyperparameters\n",
        "    merge_img_and_txt_features = concatenate([fe2, se3])\n",
        "    decoder = Dense(dense_layer_units, activation=hidden_layers_activation)(merge_img_and_txt_features)\n",
        "    outputs = Dense(vocabSize, activation='softmax')(decoder)\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    \n",
        "    # setting the Embedding layer to not be trainable, as the glove embeddings fetched from the txt file are already pre-trained results\n",
        "    model.layers[2].set_weights([embedding_matrix])\n",
        "    model.layers[2].trainable = False\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam') # to do: add any other metrics that are suitable\n",
        "\n",
        "    return model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Second Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.layers import LSTM, Embedding, Dropout, concatenate, Bidirectional\n",
        "                         \n",
        "from keras_self_attention import SeqSelfAttention\n",
        "\n",
        "def create_attention_model(embedding_matrix, vocabSize, droput_rate = 0.5, dense_layer_units = 256, hidden_layers_activation = 'relu'): # to do: add any extra hyperparameters\n",
        "    inputs1 = Input(shape=(2048,)) # 2048 is fixed, as this is an image's feature vector size returned by Google's Inception model\n",
        "    fe1 = Dropout(droput_rate)(inputs1)\n",
        "    fe2 = Dense(dense_layer_units, activation=hidden_layers_activation)(fe1)\n",
        "    inputs2 = Input(shape=(maxCapLen,))\n",
        "    se1 = Embedding(vocabSize, embedding_dim, mask_zero=True)(inputs2)\n",
        "    se2 = Dropout(droput_rate)(se1)\n",
        "    se3 = LSTM(dense_layer_units, return_sequences=True)(se2) \n",
        "    se4 = SeqSelfAttention(attention_activation='sigmoid')(se3)\n",
        "    se5 = Flatten()(se4)\n",
        "    print(se5)\n",
        "    merge_img_and_txt_features = concatenate([fe2, se5])\n",
        "    decoder = Dense(dense_layer_units, activation=hidden_layers_activation)(merge_img_and_txt_features)\n",
        "    outputs = Dense(vocabSize, activation='softmax')(decoder)\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "\n",
        "    # setting the Embedding layer to not be trainable, as the glove embeddings fetched from the txt file are already pre-trained results\n",
        "    model.layers[1].set_weights([embedding_matrix])\n",
        "    model.layers[1].trainable = False\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam') # to do: add any other metrics that are suitable\n",
        "\n",
        "    return model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preparing Model Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data generator, intended to be used in a call to model.fit_generator()\n",
        "def dataGenerator(imgToCaptions, imgToFeatures, wordToIdx, vocabSize, maxCaptionLength, imgsBatchSize):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    n = 0\n",
        "    # loop forever over images\n",
        "    while True:\n",
        "        for imgId, captions in imgToCaptions.items():\n",
        "            n += 1\n",
        "            imgFeatures = imgToFeatures[imgId+'.jpg'] # retrieve the image's feature vector\n",
        "            for caption in captions:\n",
        "                seq = [wordToIdx[word] for word in caption.split(' ') if word in wordToIdx] # encode the caption into a sequence of numbers instead of words\n",
        "                for i in range(1, len(seq)): # split one sequence into multiple X, y pairs\n",
        "                    inSeq, outSeq = seq[:i], seq[i] # split into input and output pair\n",
        "                    inSeq = preprocessing.sequence.pad_sequences([inSeq], maxlen=maxCaptionLength, padding=\"pre\")[0] # pad input sequence\n",
        "                    outSeq = to_categorical([outSeq], num_classes=vocabSize)[0] # (one-hot) encodes the output sequence (note: to_categorical() is a keras-related function)\n",
        "                    X1.append(imgFeatures) # store the values\n",
        "                    X2.append(inSeq)\n",
        "                    y.append(outSeq)\n",
        "            # yield the batch data\n",
        "            if n == imgsBatchSize:\n",
        "                yield [[np.array(X1), np.array(X2)], np.array(y)] # \"yield\" saves the function's state, returns [[..]], then continues function from that statement when function is called again\n",
        "                X1, X2, y = list(), list(), list()\n",
        "                n = 0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Explanation of inner-most for loop above:\n",
        "\n",
        "<img src=\"project_media/xi_as_words.png\" width=\"600\" />\n",
        "\n",
        "However, since we're using `wordToIdx` mapping, the table above will be:\n",
        "\n",
        "<img src=\"project_media/xi_as_idxss.png\" width=\"600\" />\n",
        "\n",
        "Note 1: the table above is for the case of `post` padding. However, we'll assume `pre` padding, as it is [generally advised](https://stackoverflow.com/questions/46298793/how-does-choosing-between-pre-and-post-zero-padding-of-sequences-impact-results#:~:text=I%20always%20recommend%20using%20pre%2Dpadding%20over%20post%2Dpadding%2C%20even%20for%20CNNs%2C%20unless%20the%20problem%20specifically%20requires%20post%2Dpadding.)\n",
        "\n",
        "Note 2: under the hood, `target word` is one-hot encoded representation of the numerical values displayed in the table above; this is because one-hot encoding the target word allows us to represent this probability distribution as a vector of probabilities over the entire vocabulary, which can be directly compared to the predicted probability distribution output by the neural network  \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original vocabulary (i.e., unique words) size: 9630\n",
            "Vocabulary size after removing less frequent words (< 5 words): 3107\n",
            "number of words with no embeddings: 15\n",
            "which are:\n",
            "['surfboarder', 'plushie', 'corndogs', 'waterskier', 'wakeboards', 'dalmation', 'inground', 'rollerblader', 'bicycler', 'floaties', 'wakeboarder', 'parasails', 'outstreached', 'windsurfs', 'somthing']\n",
            "embedding matrix dimensions (vocab size x embedding dimensions):\n",
            "(3108, 200)\n"
          ]
        }
      ],
      "source": [
        "# hyperparameters:\n",
        "freq_threshold = 5\n",
        "inception_model = InceptionV3(weights='imagenet') # getting the InceptionV3 model trained on imagenet data\n",
        "model_for_feature_extraction = Model(inception_model.input, inception_model.layers[-2].output) # removing the last layer (output softmax layer)\n",
        "imgs_batch_size = 32\n",
        "epochs = 10\n",
        "embedding_dim = 200\n",
        "\n",
        "(imgToCaptions, vocab, vocabWordFreq, vocabWordFreqRemoved, \n",
        "idxToWord, wordToIdx, vocabSize, maxCapLen,\n",
        "embedding_matrix, words_with_no_embedding,\n",
        "trainImgsIds, valImgsIds, testImgsIds, \n",
        "trainImgToFeatures, valImgToFeatures, testImgToFeatures, \n",
        "trainImgToCaptions, valImgToCaptions, testImgToCaptions) = preprocessing_pipeline(freq_threshold, model_for_feature_extraction, embedding_dim=embedding_dim, load_features=True) # set load_features=True if you won't change \"weights\" argument of \"model\" variable to avoid bottleneck of using Google's Inception Model\n",
        "\n",
        "train_datagen = dataGenerator(trainImgToCaptions, trainImgToFeatures, wordToIdx, vocabSize, maxCapLen, imgs_batch_size)\n",
        "val_datagen = dataGenerator(valImgToCaptions, valImgToFeatures, wordToIdx, vocabSize, maxCapLen, imgs_batch_size)\n",
        "test_datagen = dataGenerator(testImgToCaptions, testImgToFeatures, wordToIdx, vocabSize, maxCapLen, imgs_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model hyperparameters:\n",
        "droput_rate = 0.5\n",
        "dense_layer_units = 256\n",
        "hidden_layers_activation = 'relu'\n",
        "# to do: add any extra hyperparameters\n",
        "\n",
        "lstm_model = create_lstm_model(embedding_matrix, vocabSize, droput_rate, dense_layer_units, hidden_layers_activation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "187/187 - 221s - loss: 4.8974 - 221s/epoch - 1s/step\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17584\\946929326.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtrain_datagen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainImgToCaptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainImgToFeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordToIdx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxCapLen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgs_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mlstm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_datagen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mlstm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'models/lstm_weights/lstm_model_with_{str(i)}_epochs.h5'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#  to do: change file name to include other hyperparameters as well\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Shehab\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Shehab\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                         ):\n\u001b[0;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1565\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m       (graph_function,\n\u001b[0;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2497\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# hyperparameters:\n",
        "steps = len(trainImgToCaptions) // imgs_batch_size\n",
        "\n",
        "for i in range(1, epochs+1):\n",
        "    train_datagen = dataGenerator(trainImgToCaptions, trainImgToFeatures, wordToIdx, vocabSize, maxCapLen, imgs_batch_size)\n",
        "    lstm_model.fit(train_datagen, epochs=epochs, steps_per_epoch=steps, verbose=2)\n",
        "    lstm_model.save_weights(f'models/lstm_weights/lstm_model_with_{str(i)}_epochs.h5') #  to do: change file name to include other hyperparameters as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# after training the model, you can comment the lines above, and load the model using:\n",
        "lstm_model.load_weights(f'models/second_weights/second_model_with_{epochs}_epochs.h5')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Second Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original vocabulary (i.e., unique words) size: 9630\n",
            "Vocabulary size after removing less frequent words (< 5 words): 3107\n",
            "number of words with no embeddings: 200\n",
            "which are:\n",
            "['Race', 'Hiker', 'Eight', 'windsurfs', 'parasails', 'Speedo', 'Doberman', 'Green', 'Couple', 'Skiers', 'Mickey', 'Bikers', 'Mohawk', 'Biker', 'Skiiers', 'Seattle', 'Skier', 'One', 'Boy', 'Jesus', 'Pizza', 'inground', 'Basketball', 'Older', 'Naked', 'Chinese', 'Eastern', 'Blue', 'Michael', 'Boys', 'Seven', 'Light', 'Crowd', 'Terrier', 'Legos', 'Greyhounds', 'Teenagers', 'Four', 'On', 'Boston', 'Women', 'Adults', 'Female', 'Jack', 'BMX', 'Red', 'Street', 'Toddler', 'Christmas', 'Asian', 'Indian', 'Tennis', 'Grey', 'People', 'Brown', 'In', 'Children', 'surfboarder', 'Three', 'Dirt', 'St', 'Wet', 'outstreached', 'White', 'Irish', 'Miami', 'I', 'German', 'Girl', 'Baby', 'Kids', 'somthing', 'Teen', 'TV', 'Six', 'Distant', 'The', 'Blonde', 'Five', 'Easter', 'An', 'Snowboarder', 'ATV', 'Hockey', 'Bike', 'Many', 'Blond', 'While', 'Mouse', 'Collie', 'Group', 'Number', 'Black', 'Person', 'Men', 'T-shirt', 'Yellow', 'York', 'Small', 'Man', 'Cyclists', 'waterskier', 'DJ', 'Batman', 'wakeboards', 'Surfer', 'European', 'Santa', 'Claus', 'Someone', 'Jeep', 'Jockeys', 'Obama', 'Sheltie', 'These', 'Labrador', 'Dog', 'RV', 'Golden', 'corndogs', 'Mountain', 'wakeboarder', 'There', 'Rollerblades', 'Orange', 'floaties', 'mid-jump', 'Young', 'Muzzled', 'Closeup', 'Dark', 'Mexican', 'hula-hoops', 'Kid', 'Baseball', 'New', 'During', 'Spectators', 'Spider-Man', 'Japanese', 'Dogs', 'Big', 'Snow', 'Shephard', 'University', 'They', 'Cyclist', 'Greyhound', 'Florida', 'Some', 'Oklahoma', 'Shirtless', 'plushie', 'Where', 'Lady', 'Soccer', 'rollerblader', 'Girls', 'Domino', 'A', 'SUV', 'Armenian', 'Water', 'ATM', 'Marx', 'Jackson', 'Hawaiian', 'African', 'Halloween', 'Woman', 'Football', 'Middle', 'Frisbee', 'Child', 'tri-colored', 'Large', 'Two', 'Shepherd', 'Family', 'Racing', 'With', 'Smiling', 'Washington', 'This', 'Free', 'Guy', 'Sooners', 'Beautiful', 'American', 'Rugby', 'Skateboarder', 'Several', 'bicycler', 'Tan', 'Little', 'Old', 'Spiderman', 'At', 'Dalmation', 'hi-viz']\n",
            "embedding matrix dimensions (vocab size x embedding dimensions):\n",
            "(3108, 200)\n"
          ]
        }
      ],
      "source": [
        "# hyperparameters to tune\n",
        "freq_threshold = 5\n",
        "inception_model = InceptionV3(weights='imagenet') # getting the InceptionV3 model trained on imagenet data\n",
        "model_for_feature_extraction = Model(inception_model.input, inception_model.layers[-2].output) # removing the last layer (output softmax layer)\n",
        "imgs_batch_size = 32\n",
        "epochs = 10\n",
        "embedding_dim = 200\n",
        "\n",
        "(imgToCaptions, vocab, vocabWordFreq, vocabWordFreqRemoved, \n",
        "idxToWord, wordToIdx, vocabSize, maxCapLen,\n",
        "embedding_matrix, words_with_no_embedding,\n",
        "trainImgsIds, valImgsIds, testImgsIds, \n",
        "trainImgToFeatures, valImgToFeatures, testImgToFeatures, \n",
        "trainImgToCaptions, valImgToCaptions, testImgToCaptions) = preprocessing_pipeline(freq_threshold, model_for_feature_extraction, embedding_dim=embedding_dim, load_features=True) # set load_features=True if you won't change \"weights\" argument of \"model\" variable to avoid bottleneck of using Google's Inception Model\n",
        "\n",
        "train_datagen = dataGenerator(trainImgToCaptions, trainImgToFeatures, wordToIdx, vocabSize, maxCapLen, imgs_batch_size)\n",
        "val_datagen = dataGenerator(valImgToCaptions, valImgToFeatures, wordToIdx, vocabSize, maxCapLen, imgs_batch_size)\n",
        "test_datagen = dataGenerator(testImgToCaptions, testImgToFeatures, wordToIdx, vocabSize, maxCapLen, imgs_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 10240), dtype=tf.float32, name=None), name='flatten_4/Reshape:0', description=\"created by layer 'flatten_4'\")\n"
          ]
        }
      ],
      "source": [
        "# model hyperparameters:\n",
        "droput_rate = 0.5\n",
        "dense_layer_units = 256\n",
        "hidden_layers_activation = 'relu'\n",
        "# to do: add any extra hyperparameters\n",
        "\n",
        "second_model = create_attention_model(embedding_matrix, vocabSize, droput_rate, dense_layer_units, hidden_layers_activation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "in user code:\n\n    d:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\keras\\engine\\training.py:853 train_function  *\n        return step_function(self, iterator)\n    d:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\keras\\engine\\training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    d:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    d:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    d:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    d:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\keras\\engine\\training.py:835 run_step  **\n        outputs = model.train_step(data)\n    d:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\keras\\engine\\training.py:787 train_step\n        y_pred = self(x, training=True)\n    d:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\keras\\engine\\base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    d:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\keras\\engine\\input_spec.py:199 assert_input_compatibility\n        raise ValueError('Layer ' + layer_name + ' expects ' +\n\n    ValueError: Layer model_29 expects 2 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, None) dtype=int32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, None) dtype=float32>]\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[388], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m      5\u001b[0m     train_datagen \u001b[39m=\u001b[39m dataGenerator(trainImgToCaptions, trainImgToFeatures, wordToIdx, vocabSize, maxCapLen, imgs_batch_size)\n\u001b[1;32m----> 6\u001b[0m     second_model\u001b[39m.\u001b[39;49mfit(train_datagen, epochs\u001b[39m=\u001b[39;49mepochs, steps_per_epoch\u001b[39m=\u001b[39;49msteps, verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m     second_model\u001b[39m.\u001b[39msave_weights(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodels/second_weights/second_model_with_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(i)\u001b[39m}\u001b[39;00m\u001b[39m_epochs.h5\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m# to do: change \"second\", and change file name to include other hyperparameters as well\u001b[39;00m\n",
            "File \u001b[1;32md:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\keras\\engine\\training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1178\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1179\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1180\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1181\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1182\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1183\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1184\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1185\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1186\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[1;32md:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32md:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:924\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    922\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    923\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 924\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    925\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    926\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    927\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32md:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3038\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3035\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[39;00m\n\u001b[0;32m   3036\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   3037\u001b[0m   (graph_function,\n\u001b[1;32m-> 3038\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[0;32m   3039\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39m_call_flat(\n\u001b[0;32m   3040\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39mgraph_function\u001b[39m.\u001b[39mcaptured_inputs)\n",
            "File \u001b[1;32md:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3459\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3449\u001b[0m \u001b[39mwith\u001b[39;00m ag_ctx\u001b[39m.\u001b[39mControlStatusCtx(\n\u001b[0;32m   3450\u001b[0m     status\u001b[39m=\u001b[39mag_status, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autograph_options):\n\u001b[0;32m   3451\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3454\u001b[0m   \u001b[39m# and 2. there's no provided input signature\u001b[39;00m\n\u001b[0;32m   3455\u001b[0m   \u001b[39m# and 3. there's been a cache miss for this calling context\u001b[39;00m\n\u001b[0;32m   3456\u001b[0m   \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experimental_relax_shapes \u001b[39mand\u001b[39;00m\n\u001b[0;32m   3457\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_signature \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m   3458\u001b[0m       call_context_key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39mmissed):\n\u001b[1;32m-> 3459\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_define_function_with_shape_relaxation(\n\u001b[0;32m   3460\u001b[0m         args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0;32m   3462\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39mmissed\u001b[39m.\u001b[39madd(call_context_key)\n\u001b[0;32m   3463\u001b[0m   graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_graph_function(args, kwargs)\n",
            "File \u001b[1;32md:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3381\u001b[0m, in \u001b[0;36mFunction._define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[0;32m   3374\u001b[0m   (relaxed_arg_specs, relaxed_kwarg_specs) \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mpack_sequence_as(\n\u001b[0;32m   3375\u001b[0m       (args, kwargs), relaxed_arg_specs, expand_composites\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   3376\u001b[0m   (args, kwargs) \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mpack_sequence_as(\n\u001b[0;32m   3377\u001b[0m       (relaxed_arg_specs, relaxed_kwarg_specs),\n\u001b[0;32m   3378\u001b[0m       flat_args,\n\u001b[0;32m   3379\u001b[0m       expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m-> 3381\u001b[0m graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_graph_function(\n\u001b[0;32m   3382\u001b[0m     args, kwargs, override_flat_arg_shapes\u001b[39m=\u001b[39;49mrelaxed_arg_shapes)\n\u001b[0;32m   3383\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39marg_relaxed[rank_only_cache_key] \u001b[39m=\u001b[39m graph_function\n\u001b[0;32m   3385\u001b[0m \u001b[39mreturn\u001b[39;00m (graph_function, [\n\u001b[0;32m   3386\u001b[0m     t \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m nest\u001b[39m.\u001b[39mflatten((args, kwargs), expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   3387\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (ops\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   3388\u001b[0m                       resource_variable_ops\u001b[39m.\u001b[39mBaseResourceVariable))\n\u001b[0;32m   3389\u001b[0m ])\n",
            "File \u001b[1;32md:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3298\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3293\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[0;32m   3294\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   3295\u001b[0m ]\n\u001b[0;32m   3296\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[0;32m   3297\u001b[0m graph_function \u001b[39m=\u001b[39m ConcreteFunction(\n\u001b[1;32m-> 3298\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[0;32m   3299\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[0;32m   3300\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[0;32m   3301\u001b[0m         args,\n\u001b[0;32m   3302\u001b[0m         kwargs,\n\u001b[0;32m   3303\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_signature,\n\u001b[0;32m   3304\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[0;32m   3305\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[0;32m   3306\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[0;32m   3307\u001b[0m         override_flat_arg_shapes\u001b[39m=\u001b[39;49moverride_flat_arg_shapes,\n\u001b[0;32m   3308\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[0;32m   3309\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[0;32m   3310\u001b[0m     function_spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[0;32m   3311\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m   3312\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m   3313\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m   3314\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m   3315\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   3316\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
            "File \u001b[1;32md:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1007\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1004\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1005\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1007\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39mfunc_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1009\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m func_outputs \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mmap_structure(convert, func_outputs,\n\u001b[0;32m   1012\u001b[0m                                   expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[1;32md:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:668\u001b[0m, in \u001b[0;36mFunction._defun_with_scope.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[39mwith\u001b[39;00m default_graph\u001b[39m.\u001b[39m_variable_creator_scope(scope, priority\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m):  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    665\u001b[0m   \u001b[39m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    666\u001b[0m   \u001b[39m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    667\u001b[0m   \u001b[39mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 668\u001b[0m     out \u001b[39m=\u001b[39m weak_wrapped_fn()\u001b[39m.\u001b[39m__wrapped__(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    669\u001b[0m   \u001b[39mreturn\u001b[39;00m out\n",
            "File \u001b[1;32md:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:994\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    993\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 994\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[0;32m    995\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    996\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
            "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    d:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\keras\\engine\\training.py:853 train_function  *\n        return step_function(self, iterator)\n    d:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\keras\\engine\\training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    d:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    d:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    d:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    d:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\keras\\engine\\training.py:835 run_step  **\n        outputs = model.train_step(data)\n    d:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\keras\\engine\\training.py:787 train_step\n        y_pred = self(x, training=True)\n    d:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\keras\\engine\\base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    d:\\CS\\projects\\nlp-image-captioning\\.conda\\lib\\site-packages\\keras\\engine\\input_spec.py:199 assert_input_compatibility\n        raise ValueError('Layer ' + layer_name + ' expects ' +\n\n    ValueError: Layer model_29 expects 2 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, None) dtype=int32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, None) dtype=float32>]\n"
          ]
        }
      ],
      "source": [
        "# hyperparameters:\n",
        "steps = len(trainImgToCaptions) // imgs_batch_size\n",
        "\n",
        "for i in range(1, epochs+1):\n",
        "    train_datagen = dataGenerator(trainImgToCaptions, trainImgToFeatures, wordToIdx, vocabSize, maxCapLen, imgs_batch_size)\n",
        "    second_model.fit(train_datagen, epochs=epochs, steps_per_epoch=steps, verbose=2)\n",
        "    second_model.save_weights(f'models/second_weights/second_model_with_{str(i)}_epochs.h5') # to do: change \"second\", and change file name to include other hyperparameters as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# after training the model, you can comment the lines above, and load the model using:\n",
        "second_model.load_weights(f'models/second_weights/second_model_with_{epochs}_epochs.h5')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# to do: add metrics to be used when training models\n",
        "# then, visualize and compare between models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# to do (phase 2):\n",
        "# code for embedding\n",
        "# code for model architecture\n",
        "# code for model training; provided here: vvv\n",
        "\n",
        "# epochs = 10\n",
        "# numImgsPerBatch = 3\n",
        "# steps = len(trainImgToCaptions)//numImgsPerBatch\n",
        "# for i in range(1, epochs+1):\n",
        "#     generator = dataGenerator(trainImgToCaptions, trainImgToFeatures, wordToIdx, maxCapLen, numImgsPerBatch)\n",
        "#     model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "#     model.save(f'models/changingEpochs/modelWith{str(i)}Epochs.h5')\n",
        "\n",
        "# code for validating/testing (inference)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c7cdde67bd1df58aee8e05336b4c9e41dfb37650659bf34d4c10242d0898ac5a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
